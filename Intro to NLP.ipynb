{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Agenda\n",
    "\n",
    "1. Natural Language Processing - Motivating Tasks\n",
    "2. Classical NLP\n",
    "    - Processing text for machine learning\n",
    "    - Text Classification\n",
    "    - Topic Modeling\n",
    "3. Modern NLP - State of the Art models using Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Natural Language Processing\n",
    "\n",
    "Natural Language Processing is a field combining linguistics and computer science to analyze natural language data and perform various tasks.\n",
    "\n",
    "NLP is a broad topic that covers many different tasks. Common tasks include:\n",
    "\n",
    "1. Text Classification - Predict the topic of a news article from a predefined set of topics.\n",
    "2. Named Entity Recognition - If \"apple\" is used in a sentence does it refer to fruit or a company?\n",
    "3. Question Answering - Given a context text, answer a question about it.\n",
    "4. Text Summarization - Produce a summary of a given text.\n",
    "5. Translation - Translate English to Spanish\n",
    "6. Text Generation - Given a prompt, write a story.\n",
    "7. Dialogue State Tracking - Given a conversation, record key facts about it.\n",
    "8. Topic modeling - Given a corpus of texts, discover common topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Set up Google Colab runtime\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # stop warnings for the sake of presentation\n",
    "if \"google.colab\" in sys.modules:\n",
    "    print(\"Setting up Google Colab... \")\n",
    "    !git clone https://github.com/Strabes/Intro-to-NLP.git intro-to-nlp\n",
    "    %cd intro-to-nlp\n",
    "    from install import install_requirements\n",
    "    install_requirements()\n",
    "    nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 20 Newsgroups Dataset\n",
    "\n",
    "The 20 Newsgroups dataset is a classic dataset in NLP for document classification experiments. It consists of ~20K newsgroup posts that are classified into 20 topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The topics in the dataset are: 'alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "d_train = fetch_20newsgroups(\n",
    "    subset=\"train\",\n",
    "    remove=('headers','footers','quotes'),\n",
    "    shuffle=True,\n",
    "    random_state=42)\n",
    "\n",
    "d_test = fetch_20newsgroups(\n",
    "    subset=\"test\",\n",
    "    remove=('headers','footers','quotes'),\n",
    "    shuffle=True,\n",
    "    random_state=42)\n",
    "\n",
    "print(\"The topics in the dataset are: \" +\n",
    "  \", \".join([f\"'{x}'\" for x in d_train[\"target_names\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "--------------------------------------------------\n",
      "A fair number of brave souls who upgraded their SI clock oscillator have\n",
      "shared their experiences for this poll. Please send a brief message detailing\n",
      "your experiences with the procedure. Top speed attained, CPU rated speed,\n",
      "add on cards and adapters, heat sinks, hour of usage per day, floppy disk\n",
      "functionality with 800 and 1.4 m floppies are especially requested.\n",
      "\n",
      "I will be summarizing in the next two days, so please add to the network\n",
      "knowledge base if you have done the clock upgrade and haven't answered this\n",
      "poll. Thanks.\n",
      "--------------------------------------------------\n",
      "well folks, my mac plus finally gave up the ghost this weekend after\n",
      "starting life as a 512k way back in 1985.  sooo, i'm in the market for a\n",
      "new machine a bit sooner than i intended to be...\n",
      "\n",
      "i'm looking into picking up a powerbook 160 or maybe 180 and have a bunch\n",
      "of questions that (hopefully) somebody can answer:\n",
      "\n",
      "* does anybody know any dirt on when the next round of powerbook\n",
      "introductions are expected?  i'd heard the 185c was supposed to make an\n",
      "appearence \"this summer\" but haven't heard anymore on it - and since i\n",
      "don't have access to macleak, i was wondering if anybody out there had\n",
      "more info...\n",
      "\n",
      "* has anybody heard rumors about price drops to the powerbook line like the\n",
      "ones the duo's just went through recently?\n",
      "\n",
      "* what's the impression of the display on the 180?  i could probably swing\n",
      "a 180 if i got the 80Mb disk rather than the 120, but i don't really have\n",
      "a feel for how much \"better\" the display is (yea, it looks great in the\n",
      "store, but is that all \"wow\" or is it really that good?).  could i solicit\n",
      "some opinions of people who use the 160 and 180 day-to-day on if its worth\n",
      "taking the disk size and money hit to get the active display?  (i realize\n",
      "this is a real subjective question, but i've only played around with the\n",
      "machines in a computer store breifly and figured the opinions of somebody\n",
      "who actually uses the machine daily might prove helpful).\n",
      "\n",
      "* how well does hellcats perform?  ;)\n",
      "\n",
      "thanks a bunch in advance for any info - if you could email, i'll post a\n",
      "summary (news reading time is at a premium with finals just around the\n",
      "corner... :( )\n",
      "--\n",
      "Tom Willis     twillis@ecn.purdue.edu         Purdue Electrical Engineering\n"
     ]
    }
   ],
   "source": [
    "for text in d_train[\"data\"][:3]:\n",
    "    print(\"-\"*50)\n",
    "    print(text.replace(\"\\\\\",\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "NLP requires converting natural language documents to numeric representations and performing computations on these representations. The first step to **encoding** documents into numeric representations is breaking the documents down into smaller units via **tokenization**. One obvious method of tokenization is **word tokenization**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      " ==> \n",
      "'I', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'I', 'saw', 'the', 'other', 'day', '.', 'It', 'was', 'a', '2-door', 'sports', 'car', ',', 'looked', 'to', 'be', 'from', 'the', 'late', '60s/', 'early', '70s', '.', 'It', 'was', 'called', 'a', 'Bricklin', '.', 'The', 'doors', 'were', 'really', 'small', '.', 'In', 'addition', ',', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', '.', 'This', 'is', 'all', 'I', 'know', '.', 'If', 'anyone', 'can', 'tellme', 'a', 'model', 'name', ',', 'engine', 'specs', ',', 'years', 'of', 'production', ',', 'where', 'this', 'car', 'is', 'made', ',', 'history', ',', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', ',', 'please', 'e-mail', '.'\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "example = d_train[\"data\"][0]\n",
    "\n",
    "def word_tokenize(text):\n",
    "    x = nltk.word_tokenize(text.replace(\"\\\\\",\" \"))\n",
    "    return x\n",
    "\n",
    "example_tokenized = word_tokenize(example)\n",
    "\n",
    "print(example + \"\\n ==> \")\n",
    "print(\", \".join([f\"'{t}'\" for t in example_tokenized]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We then create a **vocabulary** that maps tokens to indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0,\n",
       " 'The': 1,\n",
       " 'wondering': 2,\n",
       " 'history': 3,\n",
       " '2-door': 4,\n",
       " 'name': 5,\n",
       " 'bumper': 6,\n",
       " 'or': 7,\n",
       " 'on': 8,\n",
       " 'early': 9}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import islice\n",
    "dict(islice({j:i for i,j in enumerate(set(example_tokenized))}.items(),10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Of course we do not use just a single document to create our vocabulary, but a collection of documents, called a **corpus**. We also need to specify a maximum number of tokens for our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a subset of our vocabulary:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'I': 146,\n",
       " 'was': 948,\n",
       " 'wondering': 979,\n",
       " 'if': 572,\n",
       " 'anyone': 313,\n",
       " 'out': 731,\n",
       " 'there': 893,\n",
       " 'could': 414,\n",
       " 'me': 659,\n",
       " 'on': 718}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "count_vectorizer = CountVectorizer(\n",
    "    tokenizer = word_tokenize,\n",
    "    max_features = 1000, # max number of tokens in our vocabulary\n",
    "    lowercase = False)\n",
    "\n",
    "# Train the vocabulary on 1000 example texts\n",
    "count_vectorizer.fit(d_train[\"data\"][:1000])\n",
    "\n",
    "print(\"Here's a subset of our vocabulary:\")\n",
    "dict(islice(count_vectorizer.vocabulary_.items(),10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classical NLP\n",
    "\n",
    "Many traditional statistical and machine learning models expect data to be in a tabular format where columns correspond to specific features and rows correspond to individual observations (in the case of NLP, each document is treated as an individual observation).\n",
    "\n",
    "The most common method of transforming a corpus of documents into a tabular format is **bag-of-words** where each column in the table represents a given word (token) and the entry in row i, column j is the count of times word j occurs in document i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>!</th>\n",
       "      <th>#</th>\n",
       "      <th>$</th>\n",
       "      <th>%</th>\n",
       "      <th>&amp;</th>\n",
       "      <th>'</th>\n",
       "      <th>''</th>\n",
       "      <th>'AS</th>\n",
       "      <th>'AX</th>\n",
       "      <th>'d</th>\n",
       "      <th>...</th>\n",
       "      <th>x-Soviet</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yes</th>\n",
       "      <th>yet</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "      <th>{</th>\n",
       "      <th>|</th>\n",
       "      <th>}</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     !  #  $  %  &  '  ''  'AS  'AX  'd  ...  x-Soviet  year  years  yes  yet  \\\n",
       "0    0  0  0  0  0  0   0    0    0   0  ...         0     0      1    0    0   \n",
       "1    0  0  0  0  0  0   0    0    0   0  ...         0     0      0    0    0   \n",
       "2    0  0  0  0  0  0   3    0    0   1  ...         0     0      0    0    0   \n",
       "3    0  0  0  0  0  0   0    0    0   1  ...         0     0      0    0    0   \n",
       "4    0  0  0  0  0  2   0    0    0   0  ...         0     0      0    0    1   \n",
       "..  .. .. .. .. .. ..  ..  ...  ...  ..  ...       ...   ...    ...  ...  ...   \n",
       "995  0  0  0  0  0  0   0    0    0   0  ...         0     0      0    0    0   \n",
       "996  0  0  0  0  0  1   3    0    0   0  ...         0     0      0    0    0   \n",
       "997  0  0  0  0  0  0   0    0    0   0  ...         0     0      0    0    0   \n",
       "998  0  0  0  0  0  0   1    0    0   0  ...         0     0      0    0    0   \n",
       "999  0  0  0  0  0  0   1    0    0   1  ...         0     0      0    1    3   \n",
       "\n",
       "     you  your  {  |  }  \n",
       "0      1     0  0  0  0  \n",
       "1      1     1  0  0  0  \n",
       "2      1     0  0  0  0  \n",
       "3      1     0  0  0  0  \n",
       "4      1     0  0  0  0  \n",
       "..   ...   ... .. .. ..  \n",
       "995    0     0  0  0  0  \n",
       "996    1     0  0  0  0  \n",
       "997    0     0  0  0  0  \n",
       "998    0     0  0  2  0  \n",
       "999    2     0  0  0  0  \n",
       "\n",
       "[1000 rows x 1000 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    count_vectorizer.transform(d_train[\"data\"][:1000]).toarray(),\n",
    "    columns = count_vectorizer.get_feature_names_out().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Improved token set\n",
    "\n",
    "The simple word level tokenization above produces a number of tokens that are unlikely to be informative. Given a fixed maximum number of tokens we can generally produce a more informative set of tokens by:\n",
    "\n",
    "1. Lowercasing: no reason to produce columns for both \"Apple\" and \"apple\"\n",
    "2. Removing **stopwords**, a list of common tokens such as \"the\", \"a\", \"this\", etc. that are unlikely to add value in the bag of words approach.\n",
    "3. Reducing morphological inflections:\n",
    "   - **Stemming** uses heuristic rules to reduce morphological inflections by chopping common suffixes from tokens.\n",
    "   - **Lemmatization** is a more sophisticated technique that generally uses vocabularies, word context and part-of-speech tagging to infer the correct lemma of a word. Lemmatization tends to be more computationally intensive than stemming.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stem of 'stop' is 'stop'\n",
      "The stem of 'stops' is 'stop'\n",
      "The stem of 'stopping' is 'stop'\n",
      "The stem of 'stopped' is 'stop'\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "stemming_example_words = [\"stop\",\"stops\",\"stopping\",\"stopped\"]\n",
    "\n",
    "for word in stemming_example_words:\n",
    "    print(f\"The stem of '{word}' is '{porter_stemmer.stem(word)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>'ax</th>\n",
       "      <th>'re</th>\n",
       "      <th>'ve</th>\n",
       "      <th>145</th>\n",
       "      <th>6um</th>\n",
       "      <th>a86</th>\n",
       "      <th>ani</th>\n",
       "      <th>anoth</th>\n",
       "      <th>anyon</th>\n",
       "      <th>argument</th>\n",
       "      <th>...</th>\n",
       "      <th>use</th>\n",
       "      <th>veri</th>\n",
       "      <th>wa</th>\n",
       "      <th>want</th>\n",
       "      <th>way</th>\n",
       "      <th>whi</th>\n",
       "      <th>window</th>\n",
       "      <th>word</th>\n",
       "      <th>work</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     'ax  're  've  145  6um  a86  ani  anoth  anyon  argument  ...  use  \\\n",
       "0      0    0    0    0    0    0    0      0      2         0  ...    0   \n",
       "1      0    0    0    0    0    0    0      0      0         0  ...    0   \n",
       "2      0    0    1    0    0    0    2      0      0         0  ...    2   \n",
       "3      0    0    0    0    0    0    0      0      0         0  ...    0   \n",
       "4      0    0    0    0    0    0    0      0      0         0  ...    0   \n",
       "..   ...  ...  ...  ...  ...  ...  ...    ...    ...       ...  ...  ...   \n",
       "995    0    0    0    0    0    0    0      0      0         0  ...    0   \n",
       "996    0    0    0    0    0    0    0      0      0         0  ...    0   \n",
       "997    0    0    0    0    0    0    0      0      0         0  ...    0   \n",
       "998    0    0    0    0    0    0    0      0      0         0  ...    2   \n",
       "999    0    3    2    0    0    0    0      0      0         0  ...    7   \n",
       "\n",
       "     veri  wa  want  way  whi  window  word  work  year  \n",
       "0       0   4     0    0    0       0     0     0     1  \n",
       "1       0   0     0    0    0       0     0     0     0  \n",
       "2       0   2     0    1    0       0     0     0     0  \n",
       "3       0   0     0    0    0       0     0     0     0  \n",
       "4       0   0     0    0    0       0     0     0     0  \n",
       "..    ...  ..   ...  ...  ...     ...   ...   ...   ...  \n",
       "995     0   0     0    0    0       0     0     0     0  \n",
       "996     0   0     0    0    0       0     0     0     0  \n",
       "997     0   0     0    0    0       0     0     0     0  \n",
       "998     0   0     0    0    0       0     0     0     0  \n",
       "999     4   4     0    0    1       0     0     1     0  \n",
       "\n",
       "[1000 rows x 100 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def word_tokenize(text):\n",
    "    \"\"\"Tokenize a string by:\n",
    "    1. Word tokenizing\n",
    "    2. Filtering out tokens that don't contain at least\n",
    "       two consecutive alpha-numeric characters.\n",
    "    3. Lowercase characters\n",
    "    4. Apply one of the most common stemming algorithms,\n",
    "       the Porter stemmer\n",
    "    \"\"\"\n",
    "    x = nltk.word_tokenize(text.replace(\"\\\\\",\" \"))\n",
    "    x = [t for t in x if re.search(\"[A-Za-z0-9]{2,}\",t)]\n",
    "    x = [t.lower() for t in x]\n",
    "    return [porter_stemmer.stem(t) for t in x]\n",
    "\n",
    "count_vectorizer = CountVectorizer(\n",
    "    tokenizer=word_tokenize,\n",
    "    max_features=100,\n",
    "    stop_words='english' # use a stopword list from scikit-learn\n",
    "    )\n",
    "\n",
    "count_vectorizer.fit(d_train[\"data\"][:1000])\n",
    "\n",
    "pd.DataFrame(count_vectorizer.transform(d_train[\"data\"][:1000]).toarray(),\n",
    "columns = count_vectorizer.get_feature_names_out().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## N-grams\n",
    "\n",
    "One of the primary downsides of bag-of-words is that it loses all information contained in the order of the tokens. For example, both \"Dog bites man\" and \"Man bites dog\" produce the same bag-of-words representation. One method for partially overcoming this shortcoming is to use **n-grams**; the creation of a new token by combining `n` consecutive tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "''ax max 'ax 'ax', ''ax 'ax max', 'a86 a86 a86 a86', ''ax 'ax 'ax max', 'a86 a86 a86', 'max 'ax', 'a86 a86', 'someth like', 'doe anyon', '1d9 1d9'\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(1)\n",
    "\n",
    "count_vectorizer = CountVectorizer(\n",
    "    tokenizer=word_tokenize,\n",
    "    max_features=1000,\n",
    "    stop_words='english',\n",
    "    ngram_range=(1,4)\n",
    "    )\n",
    "\n",
    "count_vectorizer.fit(d_train[\"data\"])\n",
    "\n",
    "random_sample_n_grams = random.sample(\n",
    "    [i for i in count_vectorizer.get_feature_names() if re.search(\" \",i)],10)\n",
    "\n",
    "print(\", \".join([f\"'{t}'\" for t in random_sample_n_grams]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text Classification with Logistic Regression\n",
    "\n",
    "Finally we can use our matrix of vectorized texts to fit a logitstic regression model for topic classification. We pipe the output of our `CountVectorizer` through a `TfidfTransformer` (Term Frequency Inverse Document Frequency) object that normalizes document-level token counts by corpus-level token frequency. This is then piped into a logistic regression classifier. We fit our pipeline on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;count_vectorizer&#x27;,\n",
       "                 CountVectorizer(max_features=1000, ngram_range=(1, 4),\n",
       "                                 stop_words=&#x27;english&#x27;,\n",
       "                                 tokenizer=&lt;function word_tokenize at 0x000002A424AC7820&gt;)),\n",
       "                (&#x27;tfidf_transformer&#x27;, TfidfTransformer()),\n",
       "                (&#x27;logistic_reg&#x27;,\n",
       "                 LogisticRegression(multi_class=&#x27;multinomial&#x27;))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;count_vectorizer&#x27;,\n",
       "                 CountVectorizer(max_features=1000, ngram_range=(1, 4),\n",
       "                                 stop_words=&#x27;english&#x27;,\n",
       "                                 tokenizer=&lt;function word_tokenize at 0x000002A424AC7820&gt;)),\n",
       "                (&#x27;tfidf_transformer&#x27;, TfidfTransformer()),\n",
       "                (&#x27;logistic_reg&#x27;,\n",
       "                 LogisticRegression(multi_class=&#x27;multinomial&#x27;))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(max_features=1000, ngram_range=(1, 4), stop_words=&#x27;english&#x27;,\n",
       "                tokenizer=&lt;function word_tokenize at 0x000002A424AC7820&gt;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfTransformer</label><div class=\"sk-toggleable__content\"><pre>TfidfTransformer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(multi_class=&#x27;multinomial&#x27;)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('count_vectorizer',\n",
       "                 CountVectorizer(max_features=1000, ngram_range=(1, 4),\n",
       "                                 stop_words='english',\n",
       "                                 tokenizer=<function word_tokenize at 0x000002A424AC7820>)),\n",
       "                ('tfidf_transformer', TfidfTransformer()),\n",
       "                ('logistic_reg',\n",
       "                 LogisticRegression(multi_class='multinomial'))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"count_vectorizer\", count_vectorizer),\n",
    "    (\"tfidf_transformer\", TfidfTransformer()),\n",
    "    (\"logistic_reg\", LogisticRegression(multi_class='multinomial'))])\n",
    "\n",
    "pipe.fit(d_train[\"data\"],d_train[\"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the model using precision and recall. For a given topic:\n",
    "  - **Precision** is the number of times we predicted that topic correctly divided by the number of times we predicted that topic (both correctly and incorrectly).\n",
    "  - **Recall** is the number of times we predicted that topic correctly divided by the number of actual cases of the topic.\n",
    "\n",
    "A model that just guessed randomly and uniformly on our dataset would have both precision and recall close to 0.05. Our simple model is already much better than that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for classifier:\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.39      0.42      0.40       319\n",
      "           comp.graphics       0.49      0.55      0.52       389\n",
      " comp.os.ms-windows.misc       0.53      0.48      0.50       394\n",
      "comp.sys.ibm.pc.hardware       0.52      0.50      0.51       392\n",
      "   comp.sys.mac.hardware       0.55      0.53      0.54       385\n",
      "          comp.windows.x       0.63      0.54      0.58       395\n",
      "            misc.forsale       0.68      0.72      0.70       390\n",
      "               rec.autos       0.58      0.55      0.57       396\n",
      "         rec.motorcycles       0.36      0.57      0.44       398\n",
      "      rec.sport.baseball       0.57      0.57      0.57       397\n",
      "        rec.sport.hockey       0.70      0.70      0.70       399\n",
      "               sci.crypt       0.72      0.57      0.64       396\n",
      "         sci.electronics       0.41      0.44      0.42       393\n",
      "                 sci.med       0.60      0.63      0.61       396\n",
      "               sci.space       0.60      0.57      0.58       394\n",
      "  soc.religion.christian       0.57      0.66      0.61       398\n",
      "      talk.politics.guns       0.47      0.52      0.50       364\n",
      "   talk.politics.mideast       0.75      0.64      0.69       376\n",
      "      talk.politics.misc       0.43      0.36      0.39       310\n",
      "      talk.religion.misc       0.29      0.14      0.19       251\n",
      "\n",
      "                accuracy                           0.54      7532\n",
      "               macro avg       0.54      0.53      0.53      7532\n",
      "            weighted avg       0.55      0.54      0.54      7532\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "test_preds = pipe.predict(d_test[\"data\"])\n",
    "\n",
    "print(\n",
    "    f\"Classification report for classifier:\\n\" +\n",
    "    f\"\"\"{classification_report(\n",
    "        d_test['target'], test_preds,\n",
    "        target_names=d_test['target_names'])}\\n\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## State-of-the-Art NLP with Transformers\n",
    "\n",
    "All state-of-the-art models in NLP are now a type of deep neural network using a \"transformer\" architecture.\n",
    "\n",
    "The transformer architecture was originally introduced in the paper [Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf) by Vaswani et al. 2017 (Google Brain/Google Research).\n",
    "\n",
    "\n",
    "![Transformer Architecture](images/transformer_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Attention is All You Need* was concerned with translation, for instance from English to German or English to French. In the case of English to German, the inputs to the Transformer Encoder were the full sequence of English token indices from a document, $\\mathbb{x} = (x_1, x_2, ... , x_n)$. The output of the Transformer Encoder were a corresponding sequence of vectors $\\mathbb{z} = (z_1, z_2, ... , z_n)$.\n",
    "\n",
    "The Transformer Decoder then generates an output sequence $\\mathbb{y} = (y_1, y_2, ... , y_m)$ of German token indices one at a time. At each time step the Transformer Decoder takes the Transformer Encoder output $\\mathbb{z}$ and the part of $\\mathbb{y}$ that it had previously generated ($(y_1, ..., y_k)$ where $k<m$) and produces the next token $y_{k+1}$\n",
    "\n",
    "The Transformer was trained on several million pairs of English-German (or English-French) sentence pairs using backpropogation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT - Generative Pre-Training\n",
    "\n",
    "In [Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf), Radford et al 2018 (OpenAI), used just the Transformer Decoder and introduced a novel two-stage training paradigm:\n",
    "\n",
    "**Pretraining (Unsupervised)**: Train the model on a large corpus of unlabeled text data (typically scraped from the web). Given a chunk of text as input, the model's objective is to predict the word that came next in the source text. For instance, the model receives the input `[\"The\", \"cat\", \"is\", \"on\", \"the\"]` and must predict which word came next. Pretraining allows the model to learn general features of the language.\n",
    "\n",
    "**Fine-tuning (Supervised)**: For a specific task with limited labeled training data, like classification or textual entailment, initialize a similar model architecture using the weights learned from pretraining and continue training for the specific task. This requires some creativity in formatting the input data to the model and attaching an additional output layer that is task-specific.\n",
    "\n",
    "![GPT Pretraining and Fine-tuning](images/gpt_objectives.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer Decoder models:\n",
    "\n",
    "- OpenAI's GPT, GPT-2, GPT-3 and GPT-Neo\n",
    "- Google's PaLM and LaMDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT - Bidirectional Encoder Representations from Transformers\n",
    "\n",
    "Even though many languages are written and read left to right, the meaning of words and concepts in sentences flow in both directions. The Transformer Decoder-only models do not take advantage of this bidirectionality. Consider the sentence: \"The bat that flew through the night air almost hit the player on deck.\" If you only saw the first 8 words (\"The bat that flew through the night air\") you probably have a very different understanding of \"bat\" than if you saw the full sentence. The correct meaning of the word \"bat\" in this sentence flows backwords from \"the player on deck\".\n",
    "\n",
    "In [BERT: Pre-training of Deep Bidirectional Transformers for\n",
    "Language Understanding](https://arxiv.org/pdf/1810.04805.pdf) (Devlin et al. 2019 - also at Google) took just the Transformer Encoder from *Attention is All You Need* and extended the pretraining/fine-tuning approach to produced models that take advantage of bidirectionality and that achieved state-of-the-art performance.\n",
    "\n",
    "Pretraining:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "interpreter": {
   "hash": "cc2077442d658f45778fa1e23b0ce166896ca90401b2e33d9c328547ed2859af"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
