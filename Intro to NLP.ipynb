{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Agenda\n",
    "\n",
    "1. Natural Language Processing - Motivating Tasks\n",
    "2. Classical NLP\n",
    "    - Processing text for machine learning\n",
    "    - Text Classification\n",
    "    - Topic Modeling\n",
    "3. Modern NLP - State of the Art models using Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Natural Language Processing\n",
    "\n",
    "Natural Language Processing is a field combining linguistics and computer science to analyze natural language data and perform various tasks.\n",
    "\n",
    "NLP is a broad topic that covers many different tasks. Common tasks include:\n",
    "\n",
    "1. Text Classification - Predict the topic of a news article from a predefined set of topics.\n",
    "2. Named Entity Recognition - If \"apple\" is used in a sentence does it refer to fruit or a company?\n",
    "3. Question Answering - Given a context text, answer a question about it. This can take the form of either *extractive* question answering (highlighting a span of the input text that contains the answer) or *abstractive* question answering (generating a freeform answer).\n",
    "4. Text Summarization - Produce a summary of a given text.\n",
    "5. Textual Entailment - Given a premise and hypothesis determine if the hypothesis follows from the premise.\n",
    "6. Translation - Translate English to Spanish.\n",
    "7. Text Generation - Given a prompt, write a story.\n",
    "8. Dialogue State Tracking - Given a conversation, record key facts about it.\n",
    "9. Topic modeling - Given a corpus of texts, discover common topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Set up Google Colab runtime\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # stop warnings for the sake of presentation\n",
    "if \"google.colab\" in sys.modules:\n",
    "    print(\"Setting up Google Colab... \")\n",
    "    !git clone https://github.com/Strabes/Intro-to-NLP.git intro-to-nlp\n",
    "    %cd intro-to-nlp\n",
    "    from install import install_requirements\n",
    "    install_requirements()\n",
    "    nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 20 Newsgroups Dataset\n",
    "\n",
    "The 20 Newsgroups dataset is a classic dataset in NLP for document classification experiments. It consists of ~20K newsgroup posts that are classified into 20 topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The topics in the dataset are: 'alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "d_train = fetch_20newsgroups(\n",
    "    subset=\"train\",\n",
    "    remove=('headers','footers','quotes'),\n",
    "    shuffle=True,\n",
    "    random_state=42)\n",
    "\n",
    "d_test = fetch_20newsgroups(\n",
    "    subset=\"test\",\n",
    "    remove=('headers','footers','quotes'),\n",
    "    shuffle=True,\n",
    "    random_state=42)\n",
    "\n",
    "print(\"The topics in the dataset are: \" +\n",
    "  \", \".join([f\"'{x}'\" for x in d_train[\"target_names\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "--------------------------------------------------\n",
      "A fair number of brave souls who upgraded their SI clock oscillator have\n",
      "shared their experiences for this poll. Please send a brief message detailing\n",
      "your experiences with the procedure. Top speed attained, CPU rated speed,\n",
      "add on cards and adapters, heat sinks, hour of usage per day, floppy disk\n",
      "functionality with 800 and 1.4 m floppies are especially requested.\n",
      "\n",
      "I will be summarizing in the next two days, so please add to the network\n",
      "knowledge base if you have done the clock upgrade and haven't answered this\n",
      "poll. Thanks.\n",
      "--------------------------------------------------\n",
      "well folks, my mac plus finally gave up the ghost this weekend after\n",
      "starting life as a 512k way back in 1985.  sooo, i'm in the market for a\n",
      "new machine a bit sooner than i intended to be...\n",
      "\n",
      "i'm looking into picking up a powerbook 160 or maybe 180 and have a bunch\n",
      "of questions that (hopefully) somebody can answer:\n",
      "\n",
      "* does anybody know any dirt on when the next round of powerbook\n",
      "introductions are expected?  i'd heard the 185c was supposed to make an\n",
      "appearence \"this summer\" but haven't heard anymore on it - and since i\n",
      "don't have access to macleak, i was wondering if anybody out there had\n",
      "more info...\n",
      "\n",
      "* has anybody heard rumors about price drops to the powerbook line like the\n",
      "ones the duo's just went through recently?\n",
      "\n",
      "* what's the impression of the display on the 180?  i could probably swing\n",
      "a 180 if i got the 80Mb disk rather than the 120, but i don't really have\n",
      "a feel for how much \"better\" the display is (yea, it looks great in the\n",
      "store, but is that all \"wow\" or is it really that good?).  could i solicit\n",
      "some opinions of people who use the 160 and 180 day-to-day on if its worth\n",
      "taking the disk size and money hit to get the active display?  (i realize\n",
      "this is a real subjective question, but i've only played around with the\n",
      "machines in a computer store breifly and figured the opinions of somebody\n",
      "who actually uses the machine daily might prove helpful).\n",
      "\n",
      "* how well does hellcats perform?  ;)\n",
      "\n",
      "thanks a bunch in advance for any info - if you could email, i'll post a\n",
      "summary (news reading time is at a premium with finals just around the\n",
      "corner... :( )\n",
      "--\n",
      "Tom Willis     twillis@ecn.purdue.edu         Purdue Electrical Engineering\n"
     ]
    }
   ],
   "source": [
    "for text in d_train[\"data\"][:3]:\n",
    "    print(\"-\"*50)\n",
    "    print(text.replace(\"\\\\\",\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "NLP requires converting natural language documents to numeric representations and performing computations on these representations. The first step to **encoding** documents into numeric representations is breaking the documents down into smaller units via **tokenization**. One obvious method of tokenization is **word tokenization**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      " ==> \n",
      "'I', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'I', 'saw', 'the', 'other', 'day', '.', 'It', 'was', 'a', '2-door', 'sports', 'car', ',', 'looked', 'to', 'be', 'from', 'the', 'late', '60s/', 'early', '70s', '.', 'It', 'was', 'called', 'a', 'Bricklin', '.', 'The', 'doors', 'were', 'really', 'small', '.', 'In', 'addition', ',', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', '.', 'This', 'is', 'all', 'I', 'know', '.', 'If', 'anyone', 'can', 'tellme', 'a', 'model', 'name', ',', 'engine', 'specs', ',', 'years', 'of', 'production', ',', 'where', 'this', 'car', 'is', 'made', ',', 'history', ',', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', ',', 'please', 'e-mail', '.'\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "example = d_train[\"data\"][0]\n",
    "\n",
    "def word_tokenize(text):\n",
    "    x = nltk.word_tokenize(text.replace(\"\\\\\",\" \"))\n",
    "    return x\n",
    "\n",
    "example_tokenized = word_tokenize(example)\n",
    "\n",
    "print(example + \"\\n ==> \")\n",
    "print(\", \".join([f\"'{t}'\" for t in example_tokenized]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We then create a **vocabulary** that maps tokens to indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'looking': 0,\n",
       " 'In': 1,\n",
       " 'of': 2,\n",
       " '2-door': 3,\n",
       " 'have': 4,\n",
       " 'name': 5,\n",
       " 'saw': 6,\n",
       " '60s/': 7,\n",
       " 'early': 8,\n",
       " 'Bricklin': 9}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import islice\n",
    "dict(islice({j:i for i,j in enumerate(set(example_tokenized))}.items(),10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Of course we do not use just a single document to create our vocabulary, but a collection of documents, called a **corpus**. We also need to specify a maximum number of tokens for our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\grego\\Anaconda3\\envs\\py39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:524: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a subset of our vocabulary:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'I': 146,\n",
       " 'was': 948,\n",
       " 'wondering': 979,\n",
       " 'if': 572,\n",
       " 'anyone': 313,\n",
       " 'out': 731,\n",
       " 'there': 893,\n",
       " 'could': 414,\n",
       " 'me': 659,\n",
       " 'on': 718}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "count_vectorizer = CountVectorizer(\n",
    "    tokenizer = word_tokenize,\n",
    "    max_features = 1000, # max number of tokens in our vocabulary\n",
    "    lowercase = False)\n",
    "\n",
    "# Train the vocabulary on 1000 example texts\n",
    "count_vectorizer.fit(d_train[\"data\"][:1000])\n",
    "\n",
    "print(\"Here's a subset of our vocabulary:\")\n",
    "dict(islice(count_vectorizer.vocabulary_.items(),10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classical NLP\n",
    "\n",
    "Many traditional statistical and machine learning models expect data to be in a tabular format where columns correspond to specific features and rows correspond to individual observations (in the case of NLP, each document is treated as an individual observation).\n",
    "\n",
    "The most common method of transforming a corpus of documents into a tabular format is **bag-of-words** where each column in the table represents a given word (token) and the entry in row i, column j is the count of times word j occurs in document i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>!</th>\n",
       "      <th>#</th>\n",
       "      <th>$</th>\n",
       "      <th>%</th>\n",
       "      <th>&amp;</th>\n",
       "      <th>'</th>\n",
       "      <th>''</th>\n",
       "      <th>'AS</th>\n",
       "      <th>'AX</th>\n",
       "      <th>'d</th>\n",
       "      <th>...</th>\n",
       "      <th>x-Soviet</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yes</th>\n",
       "      <th>yet</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "      <th>{</th>\n",
       "      <th>|</th>\n",
       "      <th>}</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     !  #  $  %  &  '  ''  'AS  'AX  'd  ...  x-Soviet  year  years  yes  yet  \\\n",
       "0    0  0  0  0  0  0   0    0    0   0  ...         0     0      1    0    0   \n",
       "1    0  0  0  0  0  0   0    0    0   0  ...         0     0      0    0    0   \n",
       "2    0  0  0  0  0  0   3    0    0   1  ...         0     0      0    0    0   \n",
       "3    0  0  0  0  0  0   0    0    0   1  ...         0     0      0    0    0   \n",
       "4    0  0  0  0  0  2   0    0    0   0  ...         0     0      0    0    1   \n",
       "..  .. .. .. .. .. ..  ..  ...  ...  ..  ...       ...   ...    ...  ...  ...   \n",
       "995  0  0  0  0  0  0   0    0    0   0  ...         0     0      0    0    0   \n",
       "996  0  0  0  0  0  1   3    0    0   0  ...         0     0      0    0    0   \n",
       "997  0  0  0  0  0  0   0    0    0   0  ...         0     0      0    0    0   \n",
       "998  0  0  0  0  0  0   1    0    0   0  ...         0     0      0    0    0   \n",
       "999  0  0  0  0  0  0   1    0    0   1  ...         0     0      0    1    3   \n",
       "\n",
       "     you  your  {  |  }  \n",
       "0      1     0  0  0  0  \n",
       "1      1     1  0  0  0  \n",
       "2      1     0  0  0  0  \n",
       "3      1     0  0  0  0  \n",
       "4      1     0  0  0  0  \n",
       "..   ...   ... .. .. ..  \n",
       "995    0     0  0  0  0  \n",
       "996    1     0  0  0  0  \n",
       "997    0     0  0  0  0  \n",
       "998    0     0  0  2  0  \n",
       "999    2     0  0  0  0  \n",
       "\n",
       "[1000 rows x 1000 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    count_vectorizer.transform(d_train[\"data\"][:1000]).toarray(),\n",
    "    columns = count_vectorizer.get_feature_names_out().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Improved token set\n",
    "\n",
    "The simple word level tokenization above produces a number of tokens that are unlikely to be informative. Given a fixed maximum number of tokens we can generally produce a more informative set of tokens by:\n",
    "\n",
    "1. Lowercasing: no reason to produce columns for both \"Apple\" and \"apple\"\n",
    "2. Removing **stopwords**, a list of common tokens such as \"the\", \"a\", \"this\", etc. that are unlikely to add value in the bag of words approach.\n",
    "3. Reducing morphological inflections:\n",
    "   - **Stemming** uses heuristic rules to reduce morphological inflections by chopping common suffixes from tokens.\n",
    "   - **Lemmatization** is a more sophisticated technique that generally uses vocabularies, word context and part-of-speech tagging to infer the correct lemma of a word. Lemmatization tends to be more computationally intensive than stemming.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the stopwords we'll remove:\n",
      "\n",
      "even, noone, etc, whereby, above, further, much, cannot, it, first, since, perhaps, not, onto, each, sixty, system, whereupon, herein, someone, go, between, whatever, becoming, do, never, being, ever, us, seeming, that, full, meanwhile, from, thus, hereupon, four, during, so, he, always, becomes, to, within, almost, call, beyond, can, towards, themselves, had, on, please, nine, off, of, such, throughout, must, nothing, whom, others, afterwards, couldnt, elsewhere, our, keep, hers, because, into, third, amount, behind, about, thru, except, the, was, below, else, no, up, whereas, something, con, through, somewhere, although, you, why, any, another, ours, over, yet, de, thereby, mill, cry, top, which, your, forty, will, whose, hence, move, fire, ten, eg, several, its, show, whole, for, empty, thick, sometimes, they, anything, nevertheless, due, everyone, were, until, every, here, alone, as, however, fifteen, few, seemed, per, seem, bottom, therein, mine, thin, when, hereafter, three, found, co, too, by, somehow, sincere, this, is, may, against, namely, been, have, and, became, part, should, those, then, front, before, cant, toward, after, either, wherein, mostly, anywhere, itself, nor, be, again, detail, next, least, without, if, i, whoever, enough, though, around, them, am, are, down, neither, side, together, un, while, these, could, name, hundred, well, fifty, bill, latter, most, own, beforehand, therefore, ltd, none, nowhere, made, along, him, all, otherwise, inc, yours, see, very, yourself, formerly, thereafter, a, there, moreover, other, thence, we, often, anyhow, put, interest, same, who, anyway, some, get, she, via, whenever, amoungst, hasnt, rather, twelve, become, nobody, eleven, indeed, more, himself, whereafter, describe, thereupon, whence, twenty, ie, in, last, would, across, whither, eight, herself, what, one, my, has, than, two, besides, both, yourselves, five, whether, latterly, or, everywhere, sometime, her, only, might, find, under, where, re, already, done, seems, their, but, amongst, at, serious, upon, an, six, still, back, ourselves, out, among, wherever, myself, give, less, former, also, how, now, anyone, once, many, hereby, take, everything, beside, his, fill, me, with\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "\n",
    "ENGLISH_STOP_WORDS = sklearn.feature_extraction._stop_words.ENGLISH_STOP_WORDS\n",
    "\n",
    "print(\"Here are the stopwords we'll remove:\\n\")\n",
    "print(\", \".join(ENGLISH_STOP_WORDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stem of 'stop' is 'stop'\n",
      "The stem of 'stops' is 'stop'\n",
      "The stem of 'stopping' is 'stop'\n",
      "The stem of 'stopped' is 'stop'\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "stemming_example_words = [\"stop\",\"stops\",\"stopping\",\"stopped\"]\n",
    "\n",
    "for word in stemming_example_words:\n",
    "    print(f\"The stem of '{word}' is '{porter_stemmer.stem(word)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>applic</th>\n",
       "      <th>argument</th>\n",
       "      <th>armenian</th>\n",
       "      <th>ask</th>\n",
       "      <th>believ</th>\n",
       "      <th>better</th>\n",
       "      <th>ca</th>\n",
       "      <th>call</th>\n",
       "      <th>car</th>\n",
       "      <th>case</th>\n",
       "      <th>...</th>\n",
       "      <th>tri</th>\n",
       "      <th>true</th>\n",
       "      <th>understand</th>\n",
       "      <th>use</th>\n",
       "      <th>want</th>\n",
       "      <th>way</th>\n",
       "      <th>window</th>\n",
       "      <th>word</th>\n",
       "      <th>work</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     applic  argument  armenian  ask  believ  better  ca  call  car  case  \\\n",
       "0         0         0         0    0       0       0   0     1    4     0   \n",
       "1         0         0         0    0       0       0   0     0    0     0   \n",
       "2         0         0         0    0       0       1   0     0    0     0   \n",
       "3         0         0         0    0       0       0   0     0    0     0   \n",
       "4         0         0         0    0       0       0   0     0    0     0   \n",
       "..      ...       ...       ...  ...     ...     ...  ..   ...  ...   ...   \n",
       "995       0         0         0    0       0       0   0     0    0     0   \n",
       "996       0         0         0    0       0       0   0     0    0     0   \n",
       "997       0         0         0    0       0       0   0     0    0     0   \n",
       "998       0         0         0    0       0       0   0     0    0     0   \n",
       "999       0         0         0    1       0       0   0     1    0     0   \n",
       "\n",
       "     ...  tri  true  understand  use  want  way  window  word  work  year  \n",
       "0    ...    0     0           0    0     0    0       0     0     0     1  \n",
       "1    ...    0     0           0    0     0    0       0     0     0     0  \n",
       "2    ...    0     0           0    2     0    1       0     0     0     0  \n",
       "3    ...    0     0           0    0     0    0       0     0     0     0  \n",
       "4    ...    0     0           1    0     0    0       0     0     0     0  \n",
       "..   ...  ...   ...         ...  ...   ...  ...     ...   ...   ...   ...  \n",
       "995  ...    0     0           0    0     0    0       0     0     0     0  \n",
       "996  ...    0     0           0    0     0    0       0     0     0     0  \n",
       "997  ...    0     0           0    0     0    0       0     0     0     0  \n",
       "998  ...    1     0           0    2     0    0       0     0     0     0  \n",
       "999  ...    1     0           0    7     0    0       0     0     1     0  \n",
       "\n",
       "[1000 rows x 100 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import sklearn\n",
    "\n",
    "def word_tokenize(text, stopwords = ENGLISH_STOP_WORDS):\n",
    "    \"\"\"Tokenize a string by:\n",
    "    1. Word tokenizing\n",
    "    2. Filtering out tokens that don't contain at least\n",
    "       two consecutive alpha-numeric characters.\n",
    "    3. Lowercase characters\n",
    "    4. Apply one of the most common stemming algorithms,\n",
    "       the Porter stemmer\n",
    "    \"\"\"\n",
    "    x = nltk.word_tokenize(text.replace(\"\\\\\",\" \"))\n",
    "    x = [t for t in x if re.search(\"[A-Za-z0-9]{2,}\",t) and not re.match(\"'.*\",t)]\n",
    "    x = [t.lower() for t in x] # lowercase\n",
    "    x = [t for t in x if t not in stopwords]\n",
    "    x = [x[i] for i in range(len(x)) if i==0 or x[i] != x[i-1]]\n",
    "    return [porter_stemmer.stem(t) for t in x]\n",
    "\n",
    "count_vectorizer = CountVectorizer(\n",
    "    #tokenizer=word_tokenize,\n",
    "    analyzer=word_tokenize,\n",
    "    max_features=100\n",
    "    )\n",
    "\n",
    "count_vectorizer.fit(d_train[\"data\"][:1000])\n",
    "\n",
    "pd.DataFrame(count_vectorizer.transform(d_train[\"data\"][:1000]).toarray(),\n",
    "columns = count_vectorizer.get_feature_names_out().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## N-grams\n",
    "\n",
    "One of the primary downsides of bag-of-words is that it loses all information contained in the order of the tokens. For example, both \"Dog bites man\" and \"Man bites dog\" produce the same bag-of-words representation. One method for partially overcoming this shortcoming is to use **n-grams**; the creation of a new token by combining `n` consecutive tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'doe know', 'serial number', 'avail anonym ftp', 'hard disk', 'disk drive', 'new york', 'make sure', 'ms. myer', 'long time', 'ftp site'\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(1)\n",
    "\n",
    "count_vectorizer = CountVectorizer(\n",
    "    tokenizer=word_tokenize,\n",
    "    #analyzer=word_tokenize,\n",
    "    max_features=2000,\n",
    "    ngram_range=(1,4),\n",
    "    max_df = 0.5,\n",
    "    min_df = 0.0001\n",
    "    )\n",
    "\n",
    "count_vectorizer.fit(d_train[\"data\"])\n",
    "\n",
    "random_sample_n_grams = random.sample(\n",
    "    [i for i in count_vectorizer.get_feature_names_out() if re.search(\" \",i)],10)\n",
    "\n",
    "print(\"Some example n-grams for n>=2 \\n\")\n",
    "\n",
    "print(\", \".join([f\"'{t}'\" for t in random_sample_n_grams]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text Classification with Logistic Regression\n",
    "\n",
    "Finally we can use our matrix of vectorized texts to fit a logitstic regression model for topic classification. We pipe the output of our `CountVectorizer` through a `TfidfTransformer` (Term Frequency Inverse Document Frequency) object that normalizes document-level token counts by corpus-level token frequency. This is then piped into a logistic regression classifier. We fit our pipeline on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;count_vectorizer&#x27;,\n",
       "                 CountVectorizer(max_df=0.5, max_features=2000, min_df=0.0001,\n",
       "                                 ngram_range=(1, 4),\n",
       "                                 tokenizer=&lt;function word_tokenize at 0x000001F3B82F50D0&gt;)),\n",
       "                (&#x27;tfidf_transformer&#x27;, TfidfTransformer()),\n",
       "                (&#x27;logistic_reg&#x27;,\n",
       "                 LogisticRegression(multi_class=&#x27;multinomial&#x27;))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;count_vectorizer&#x27;,\n",
       "                 CountVectorizer(max_df=0.5, max_features=2000, min_df=0.0001,\n",
       "                                 ngram_range=(1, 4),\n",
       "                                 tokenizer=&lt;function word_tokenize at 0x000001F3B82F50D0&gt;)),\n",
       "                (&#x27;tfidf_transformer&#x27;, TfidfTransformer()),\n",
       "                (&#x27;logistic_reg&#x27;,\n",
       "                 LogisticRegression(multi_class=&#x27;multinomial&#x27;))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(max_df=0.5, max_features=2000, min_df=0.0001,\n",
       "                ngram_range=(1, 4),\n",
       "                tokenizer=&lt;function word_tokenize at 0x000001F3B82F50D0&gt;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfTransformer</label><div class=\"sk-toggleable__content\"><pre>TfidfTransformer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(multi_class=&#x27;multinomial&#x27;)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('count_vectorizer',\n",
       "                 CountVectorizer(max_df=0.5, max_features=2000, min_df=0.0001,\n",
       "                                 ngram_range=(1, 4),\n",
       "                                 tokenizer=<function word_tokenize at 0x000001F3B82F50D0>)),\n",
       "                ('tfidf_transformer', TfidfTransformer()),\n",
       "                ('logistic_reg',\n",
       "                 LogisticRegression(multi_class='multinomial'))])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"count_vectorizer\", count_vectorizer),\n",
    "    (\"tfidf_transformer\", TfidfTransformer()),\n",
    "    (\"logistic_reg\", LogisticRegression(multi_class='multinomial'))])\n",
    "\n",
    "pipe.fit(d_train[\"data\"],d_train[\"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the model using precision and recall. For a given topic:\n",
    "  - **Precision** is the number of times we predicted that topic correctly divided by the number of times we predicted that topic (both correctly and incorrectly).\n",
    "  - **Recall** is the number of times we predicted that topic correctly divided by the number of actual cases of the topic.\n",
    "\n",
    "A model that just guessed randomly and uniformly on our dataset would have both precision and recall close to 0.05. Our simple model is already much better than that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for classifier:\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.41      0.42      0.41       319\n",
      "           comp.graphics       0.56      0.59      0.57       389\n",
      " comp.os.ms-windows.misc       0.57      0.55      0.56       394\n",
      "comp.sys.ibm.pc.hardware       0.58      0.56      0.57       392\n",
      "   comp.sys.mac.hardware       0.63      0.56      0.59       385\n",
      "          comp.windows.x       0.70      0.61      0.65       395\n",
      "            misc.forsale       0.71      0.74      0.72       390\n",
      "               rec.autos       0.66      0.62      0.64       396\n",
      "         rec.motorcycles       0.42      0.70      0.53       398\n",
      "      rec.sport.baseball       0.72      0.75      0.73       397\n",
      "        rec.sport.hockey       0.83      0.79      0.81       399\n",
      "               sci.crypt       0.81      0.63      0.71       396\n",
      "         sci.electronics       0.48      0.53      0.50       393\n",
      "                 sci.med       0.68      0.68      0.68       396\n",
      "               sci.space       0.69      0.64      0.67       394\n",
      "  soc.religion.christian       0.60      0.73      0.66       398\n",
      "      talk.politics.guns       0.51      0.58      0.54       364\n",
      "   talk.politics.mideast       0.79      0.69      0.73       376\n",
      "      talk.politics.misc       0.48      0.40      0.43       310\n",
      "      talk.religion.misc       0.32      0.16      0.21       251\n",
      "\n",
      "                accuracy                           0.61      7532\n",
      "               macro avg       0.61      0.60      0.60      7532\n",
      "            weighted avg       0.62      0.61      0.61      7532\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "test_preds = pipe.predict(d_test[\"data\"])\n",
    "\n",
    "print(\n",
    "    f\"Classification report for classifier:\\n\" +\n",
    "    f\"\"\"{classification_report(\n",
    "        d_test['target'], test_preds,\n",
    "        target_names=d_test['target_names'])}\\n\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## State-of-the-Art NLP with Transformers\n",
    "\n",
    "All state-of-the-art models in NLP are now a type of deep neural network using a variant of the architecture called a \"transformer\".\n",
    "\n",
    "The transformer architecture was originally introduced in the paper [Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf) by Vaswani et al. 2017 (Google Brain/Google Research).\n",
    "\n",
    "\n",
    "![Transformer Architecture](images/transformer_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Attention is All You Need* was concerned with translation, for instance from English to German or English to French. In the case of English to German, the inputs to the Transformer Encoder are the full sequence of English token indices from a document, $\\mathbb{x} = (x_1, x_2, ... , x_n)$. The output of the Transformer Encoder are a corresponding sequence of vectors $\\mathbb{z} = (z_1, z_2, ... , z_n)$.\n",
    "\n",
    "The Transformer Decoder then generates an output sequence $\\mathbb{y} = (y_1, y_2, ... , y_m)$ of German token indices one at a time. At each time step the Transformer Decoder takes the Transformer Encoder output $\\mathbb{z}$ and the part of $\\mathbb{y}$ that it had previously generated ($(y_1, ..., y_k)$ where $k<m$) and produces the next token $y_{k+1}$\n",
    "\n",
    "The Transformer was trained on several million English-German (or English-French) sentence pairs using backpropogation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers take over all of NLP\n",
    "\n",
    "Since the publication of *Attention is All You Need* transformers have pushed out the performance frontier in all NLP tasks. There are three main branches of the transformer family tree:\n",
    "\n",
    "- Full transformer: This is the original transformer from *Attention is All You Need*. These models excel at tasks that make use of a reference text and generate a new output text, for instance translation and abstractive summarization.\n",
    "- Decoder-only transformer: these are good at generating text without a provided reference text. Decoder-only models excel as chat bots and writing stories given a prompt.\n",
    "- Encoder-only transformer: these excel in any NLP task that does not require generating new text, for instance classification, named entity recognition and extractive question answering and summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder-only Transformers - GPT\n",
    "\n",
    "In [Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf), Radford et al 2018 (OpenAI), used just the Transformer Decoder and introduced a novel two-stage training paradigm:\n",
    "\n",
    "**Pretraining (Unsupervised)**: Train the model on a large corpus of unlabeled text data (typically scraped from the web). Given a chunk of text as input, the model's objective is to predict the word that came next in the source text. For instance, the model receives the input `[\"The\", \"cat\", \"is\", \"on\", \"the\"]` and must predict which word came next. Pretraining allows the model to learn general features of the language.\n",
    "\n",
    "**Fine-tuning (Supervised)**: For a specific task with limited labeled training data, like classification or textual entailment, initialize a similar model architecture using the weights learned from pretraining and continue training for the specific task. This requires some creativity in formatting the input data to the model and attaching an additional output layer that is task-specific.\n",
    "\n",
    "![GPT Pretraining and Fine-tuning](images/gpt_objectives.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer Decoder models excel at tasks requiring text generation, for instance chat bots. The most cited transformer decoder models include:\n",
    "\n",
    "- OpenAI's GPT, GPT-2, GPT-3 and GPT-Neo\n",
    "- Google's PaLM and LaMDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-only transformers - BERT \n",
    "\n",
    "Even though many languages are written and read left to right, the meaning of words and concepts in sentences flow in both directions. The Transformer Decoder-only models do not take advantage of this bidirectionality. Consider the sentence: \"The bat that flew through the night air almost hit the player on deck.\" If you only saw the first 8 words (\"The bat that flew through the night air\") you probably have a very different understanding of \"bat\" than if you saw the full sentence. The correct meaning of the word \"bat\" in this sentence flows backwords from \"the player on deck\".\n",
    "\n",
    "In [BERT: Pre-training of Deep Bidirectional Transformers for\n",
    "Language Understanding](https://arxiv.org/pdf/1810.04805.pdf) (Devlin et al. 2019 - also at Google) took just the Transformer Encoder from *Attention is All You Need* and extended the pretraining/fine-tuning approach to produced models that take advantage of bidirectionality and that achieved state-of-the-art performance.\n",
    "\n",
    "**Pretraining (Unsupervised)**: Just like in Radford et al., train the model on a large corpus of unlabeled text data. Given a chunk of text as input, the model's objective is to predict randomly masked words. For instance, the model receives the input `[\"The\", \"[MASK]\", \"is\", \"on\", \"[MASK]\", \"mat\"]` and must predict which words are under the `\"[MASK]\"` tokens. The original BERT paper also included another pretraining task, next sentence prediction, in which the model had to predict whether one sentence followed another in a source text. This task is generally dropped in more recent versions of BERT.\n",
    "\n",
    "**Fine-tuning (Supervised)**: The fine-tuning stage is similar to that in Radford et al.: for each task, initialize a modified architecture with weights learned from pretraining and continue to train the model on labeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Google's PEGASUS full transformer model for abstractive summarization.\n",
    "\n",
    "Now we'll do an actual demo of [Google's PEGASUS](https://arxiv.org/pdf/1912.08777.pdf) full transformer model to perform abstractive summarization.\n",
    "\n",
    "Abstractive summarization takes a reference text as input to the transformer encoder and generates a summary using the transformer decoder.\n",
    "\n",
    "Google provides a trained PEGASUS model in the [Huggingface Model Hub](https://huggingface.co/models). `Huggingface` is an opensource project that provides datasets, pretrained models and software for creating, training and using models across `torch`, `tensorflow` and `JAX`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "from datasets import load_dataset, load_metric\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN/Daily Mail Dataset\n",
    "\n",
    "We'll download the CNN/Daily Mail Dataset from Huggingface. The CNN/Daily Mail dataset is well-suited for abstractive summarization because CNN/Daily Mail themselves provide brief summaries of their articles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset cnn_dailymail (C:\\Users\\grego\\.cache\\huggingface\\datasets\\cnn_dailymail\\default\\3.0.0\\e6f7373c4552f36af359a1fc84b24352f22070483560e87f524e1730f8cf5539)\n",
      "100%|██████████| 3/3 [00:00<00:00,  8.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['article', 'highlights', 'id']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"cnn_dailymail\", version=\"3.0.0\")\n",
    "print(f\"Features: {dataset['train'].column_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of an article and it's summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Article (total length: 3612):\n",
      "\n",
      "(CNN)The Palestinian Authority officially became the 123rd member of the International Criminal Court on Wednesday, a step that gives the court jurisdiction over alleged crimes in Palestinian territories. The formal accession was marked with a ceremony at The Hague, in the Netherlands, where the court is based. The Palestinians signed the ICC's founding Rome Statute in January, when they also accepted its jurisdiction over alleged crimes committed \"in the occupied Palestinian territory, including East Jerusalem, since June 13, 2014.\" Later that month, the ICC opened a preliminary examination into the situation in Palestinian territories, paving the way for possible war crimes investigations against Israelis. As members of the court, Palestinians may be subject to counter-charges as well. Israel and the United States, neither of which is an ICC member, opposed the Palestinians' efforts to join the body. But Palestinian Foreign Minister Riad al-Malki, speaking at Wednesday's ceremony, said it was a move toward greater justice. \"As Palestine formally becomes a State Party to the Rome Statute today, the world is also a step closer to ending a long era of impunity and injustice,\" he said, according to an ICC news release. \"Indeed, today brings us closer to our shared goals of justice and peace.\" Judge Kuniko Ozaki, a vice president of the ICC, said acceding to the treaty was just the first step for the Palestinians. \"As the Rome Statute today enters into force for the State of Palestine, Palestine acquires all the rights as well as responsibilities that come with being a State Party to the Statute. These are substantive commitments, which cannot be taken lightly,\" she said. Rights group Human Rights Watch welcomed the development. \"Governments seeking to penalize Palestine for joining the ICC should immediately end their pressure, and countries that support universal acceptance of the court's treaty should speak out to welcome its membership,\" said Balkees Jarrah, international justice counsel for the group. \"What's objectionable is the attempts to undermine international justice, not Palestine's decision to join a treaty to which over 100 countries around the world are members.\" In January, when the preliminary ICC examination was opened, Israeli Prime Minister Benjamin Netanyahu described it as an outrage, saying the court was overstepping its boundaries. The United States also said it \"strongly\" disagreed with the court's decision. \"As we have said repeatedly, we do not believe that Palestine is a state and therefore we do not believe that it is eligible to join the ICC,\" the State Department said in a statement. It urged the warring sides to resolve their differences through direct negotiations. \"We will continue to oppose actions against Israel at the ICC as counterproductive to the cause of peace,\" it said. But the ICC begs to differ with the definition of a state for its purposes and refers to the territories as \"Palestine.\" While a preliminary examination is not a formal investigation, it allows the court to review evidence and determine whether to investigate suspects on both sides. Prosecutor Fatou Bensouda said her office would \"conduct its analysis in full independence and impartiality.\" The war between Israel and Hamas militants in Gaza last summer left more than 2,000 people dead. The inquiry will include alleged war crimes committed since June. The International Criminal Court was set up in 2002 to prosecute genocide, crimes against humanity and war crimes. CNN's Vasco Cotovio, Kareem Khadder and Faith Karimi contributed to this report.\n",
      "\n",
      "Summary (length: 233):\n",
      "Membership gives the ICC jurisdiction over alleged crimes committed in Palestinian territories since last June .\n",
      "Israel and the United States opposed the move, which could open the door to war crimes investigations against Israelis .\n"
     ]
    }
   ],
   "source": [
    "sample_idx = 0\n",
    "sample = dataset[\"test\"][sample_idx]\n",
    "print(f\"\"\"\n",
    "Article (total length: {len(sample[\"article\"])}):\n",
    "\"\"\")\n",
    "print(sample[\"article\"])\n",
    "print(f'\\nSummary (length: {len(sample[\"highlights\"])}):')\n",
    "print(sample[\"highlights\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = dataset[\"test\"][sample_idx][\"article\"][:2000]\n",
    "# We'll collect the generated summaries of each model in a dictionary\n",
    "summaries = {}\n",
    "\n",
    "pipe = pipeline(\"summarization\", model=\"google/pegasus-cnn_dailymail\")\n",
    "pipe_out = pipe(sample_text)\n",
    "summaries[\"pegasus\"] = pipe_out[0][\"summary_text\"].replace(\" .<n>\", \".\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Palestinian Authority officially becomes the 123rd member of the International Criminal Court.\n",
      "The move gives the court jurisdiction over alleged crimes in Palestinian territories .\n"
     ]
    }
   ],
   "source": [
    "print(summaries[\"pegasus\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded text: 8087, 107, 600, 442, 117, 8303, 26159, 10539, 111, 125, 131, 208, 114, 335, 9732, 107, 2422, 62955, 40972, 4935, 10855, 39053, 7434, 15398, 35898, 107, 1\n",
      "Decoded text: Hello, ., My, name, is, Greg, Stra, bel, and, I, ', m, a, data, scientist, ., Super, cali, frag, il, istic, exp, ial, ido, cious, ., </s>\n"
     ]
    }
   ],
   "source": [
    "encoded = pipe.tokenizer.encode(\"\"\"\n",
    "Hello. My name is Greg Strabel\n",
    "and I'm a data scientist. Supercalifragilisticexpialidocious.\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Encoded text: {', '.join([str(i) for i in encoded])}\")\n",
    "decoded = [pipe.tokenizer.decode(i) for i in encoded]\n",
    "print(f\"Decoded text: {', '.join([str(i) for i in decoded])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " '.',\n",
       " 'My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Greg',\n",
       " 'Stra',\n",
       " 'bel',\n",
       " 'and',\n",
       " 'I',\n",
       " \"'\",\n",
       " 'm',\n",
       " 'a',\n",
       " 'data',\n",
       " 'scientist',\n",
       " '.',\n",
       " '</s>']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[pipe.tokenizer.decode(i) for i in encoded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['They',\n",
       " 'were',\n",
       " 'discharged',\n",
       " '.',\n",
       " 'Super',\n",
       " 'cali',\n",
       " 'frag',\n",
       " 'il',\n",
       " 'istic',\n",
       " 'exp',\n",
       " 'ial',\n",
       " 'ido',\n",
       " 'cious',\n",
       " '.',\n",
       " '</s>']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = pipe.tokenizer.encode(\"They were discharged. Supercalifragilisticexpialidocious.\")\n",
    "[pipe.tokenizer.decode(i) for i in encoded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "interpreter": {
   "hash": "cc2077442d658f45778fa1e23b0ce166896ca90401b2e33d9c328547ed2859af"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
