<!DOCTYPE html>
<html lang="en"><head>
<script src="Intro to NLP_files/libs/clipboard/clipboard.min.js"></script>
<script src="Intro to NLP_files/libs/quarto-html/tabby.min.js"></script>
<script src="Intro to NLP_files/libs/quarto-html/popper.min.js"></script>
<script src="Intro to NLP_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="Intro to NLP_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="Intro to NLP_files/libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="Intro to NLP_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.0.37">

  <meta name="author" content="Greg Strabel">
  <meta name="dcterms.date" content="2022-07-27">
  <title>Introduction to Natural Language Processing</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="Intro to NLP_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="Intro to NLP_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="Intro to NLP_files/libs/revealjs/dist/theme/quarto.css" id="theme">
  <link href="Intro to NLP_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="Intro to NLP_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="Intro to NLP_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="Intro to NLP_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-captioned.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-captioned) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-captioned.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-captioned .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-captioned .callout-caption  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-captioned.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-captioned.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-caption {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-caption {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-caption {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-captioned .callout-body > .callout-content > :last-child {
    margin-bottom: 0.5rem;
  }

  .callout.callout-captioned .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-captioned) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-caption {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-caption {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-caption {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-caption {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-caption {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
  <script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
  <script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="center">
  <h1 class="title">Introduction to Natural Language Processing</h1>
  <p class="author">Greg Strabel</p>
  <p class="date">July 27, 2022</p>
</section>

<section id="agenda" class="slide level2">
<h2>Agenda</h2>
<ol type="1">
<li>Natural Language Processing - Motivating Tasks</li>
<li>Classical NLP
<ul>
<li>Processing text for machine learning</li>
<li>Text Classification</li>
<li>Topic Modeling</li>
</ul></li>
<li>Modern NLP - State of the Art models using Transformers</li>
</ol>
</section>
<section id="natural-language-processing" class="slide level2">
<h2>Natural Language Processing</h2>
<p>Natural Language Processing is a field combining linguistics and computer science to analyze natural language data and perform various tasks.</p>
<p>NLP is a broad topic that covers many different tasks. Common tasks include:</p>
<ol type="1">
<li>Text Classification - Predict the topic of a news article from a predefined set of topics.</li>
<li>Named Entity Recognition - If “apple” is used in a sentence does it refer to fruit or a company?</li>
<li>Question Answering - Given a context text, answer a question about it. This can take the form of either <em>extractive</em> question answering (highlighting a span of the input text that contains the answer) or <em>abstractive</em> question answering (generating a freeform answer).</li>
<li>Text Summarization - Produce a summary of a given text.</li>
<li>Textual Entailment - Given a premise and hypothesis determine if the hypothesis follows from the premise.</li>
<li>Translation - Translate English to Spanish.</li>
<li>Text Generation - Given a prompt, write a story.</li>
<li>Dialogue State Tracking - Given a conversation, record key facts about it.</li>
<li>Topic modeling - Given a corpus of texts, discover common topics.</li>
</ol>
</section>
<section id="setting-up-google-colab-runtime" class="slide level2">
<h2>Setting up Google Colab runtime</h2>
<div class="cell" data-slideshow="{&quot;slide_type&quot;:&quot;-&quot;}" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="co"># Set up Google Colab runtime</span></span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="im">import</span> sys</span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="im">import</span> warnings</span>
<span id="cb1-4"><a href="#cb1-4"></a>warnings.filterwarnings(<span class="st">"ignore"</span>) <span class="co"># stop warnings for the sake of presentation</span></span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="cf">if</span> <span class="st">"google.colab"</span> <span class="kw">in</span> sys.modules:</span>
<span id="cb1-6"><a href="#cb1-6"></a>    <span class="bu">print</span>(<span class="st">"Setting up Google Colab... "</span>)</span>
<span id="cb1-7"><a href="#cb1-7"></a>    <span class="op">!</span>git clone https:<span class="op">//</span>github.com<span class="op">/</span>Strabes<span class="op">/</span>Intro<span class="op">-</span>to<span class="op">-</span>NLP.git intro<span class="op">-</span>to<span class="op">-</span>nlp</span>
<span id="cb1-8"><a href="#cb1-8"></a>    <span class="op">%</span>cd intro<span class="op">-</span>to<span class="op">-</span>nlp</span>
<span id="cb1-9"><a href="#cb1-9"></a>    <span class="im">from</span> install <span class="im">import</span> install_requirements</span>
<span id="cb1-10"><a href="#cb1-10"></a>    install_requirements()</span>
<span id="cb1-11"><a href="#cb1-11"></a>    <span class="im">from</span> IPython.display <span class="im">import</span> HTML, display</span>
<span id="cb1-12"><a href="#cb1-12"></a></span>
<span id="cb1-13"><a href="#cb1-13"></a>    <span class="kw">def</span> set_css():</span>
<span id="cb1-14"><a href="#cb1-14"></a>        display(HTML(<span class="st">'''</span></span>
<span id="cb1-15"><a href="#cb1-15"></a><span class="st">          &lt;style&gt;</span></span>
<span id="cb1-16"><a href="#cb1-16"></a><span class="st">          pre {</span></span>
<span id="cb1-17"><a href="#cb1-17"></a><span class="st">            white-space: pre-wrap;</span></span>
<span id="cb1-18"><a href="#cb1-18"></a><span class="st">          }</span></span>
<span id="cb1-19"><a href="#cb1-19"></a><span class="st">          &lt;/style&gt;</span></span>
<span id="cb1-20"><a href="#cb1-20"></a><span class="st">          '''</span>))</span>
<span id="cb1-21"><a href="#cb1-21"></a>    get_ipython().events.register(<span class="st">'pre_run_cell'</span>, set_css)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="newsgroups-dataset" class="slide level2">
<h2>20 Newsgroups Dataset</h2>
<p>The 20 Newsgroups dataset is a classic dataset in NLP for document classification experiments. It consists of ~20K newsgroup posts that are classified into 20 topics.</p>
<div class="cell" data-slideshow="{&quot;slide_type&quot;:&quot;-&quot;}" data-execution_count="3">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> fetch_20newsgroups</span>
<span id="cb2-2"><a href="#cb2-2"></a></span>
<span id="cb2-3"><a href="#cb2-3"></a>d_train <span class="op">=</span> fetch_20newsgroups(</span>
<span id="cb2-4"><a href="#cb2-4"></a>    subset<span class="op">=</span><span class="st">"train"</span>,</span>
<span id="cb2-5"><a href="#cb2-5"></a>    remove<span class="op">=</span>(<span class="st">'headers'</span>,<span class="st">'footers'</span>,<span class="st">'quotes'</span>),</span>
<span id="cb2-6"><a href="#cb2-6"></a>    shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb2-7"><a href="#cb2-7"></a>    random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb2-8"><a href="#cb2-8"></a></span>
<span id="cb2-9"><a href="#cb2-9"></a>d_test <span class="op">=</span> fetch_20newsgroups(</span>
<span id="cb2-10"><a href="#cb2-10"></a>    subset<span class="op">=</span><span class="st">"test"</span>,</span>
<span id="cb2-11"><a href="#cb2-11"></a>    remove<span class="op">=</span>(<span class="st">'headers'</span>,<span class="st">'footers'</span>,<span class="st">'quotes'</span>),</span>
<span id="cb2-12"><a href="#cb2-12"></a>    shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb2-13"><a href="#cb2-13"></a>    random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb2-14"><a href="#cb2-14"></a></span>
<span id="cb2-15"><a href="#cb2-15"></a><span class="bu">print</span>(<span class="st">"The topics in the dataset are: "</span> <span class="op">+</span></span>
<span id="cb2-16"><a href="#cb2-16"></a>  <span class="st">", "</span>.join([<span class="ss">f"'</span><span class="sc">{</span>x<span class="sc">}</span><span class="ss">'"</span> <span class="cf">for</span> x <span class="kw">in</span> d_train[<span class="st">"target_names"</span>]]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The topics in the dataset are: 'alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc'</code></pre>
</div>
</div>
</section>
<section id="example-texts-form-20-newsgroups" class="slide level2">
<h2>Example texts form 20 Newsgroups</h2>
<div class="cell" data-slideshow="{&quot;slide_type&quot;:&quot;-&quot;}" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="cf">for</span> text <span class="kw">in</span> d_train[<span class="st">"data"</span>][:<span class="dv">3</span>]:</span>
<span id="cb4-2"><a href="#cb4-2"></a>    <span class="bu">print</span>(<span class="st">"-"</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb4-3"><a href="#cb4-3"></a>    <span class="bu">print</span>(text.replace(<span class="st">"</span><span class="ch">\\</span><span class="st">"</span>,<span class="st">" "</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>--------------------------------------------------
I was wondering if anyone out there could enlighten me on this car I saw
the other day. It was a 2-door sports car, looked to be from the late 60s/
early 70s. It was called a Bricklin. The doors were really small. In addition,
the front bumper was separate from the rest of the body. This is 
all I know. If anyone can tellme a model name, engine specs, years
of production, where this car is made, history, or whatever info you
have on this funky looking car, please e-mail.
--------------------------------------------------
A fair number of brave souls who upgraded their SI clock oscillator have
shared their experiences for this poll. Please send a brief message detailing
your experiences with the procedure. Top speed attained, CPU rated speed,
add on cards and adapters, heat sinks, hour of usage per day, floppy disk
functionality with 800 and 1.4 m floppies are especially requested.

I will be summarizing in the next two days, so please add to the network
knowledge base if you have done the clock upgrade and haven't answered this
poll. Thanks.
--------------------------------------------------
well folks, my mac plus finally gave up the ghost this weekend after
starting life as a 512k way back in 1985.  sooo, i'm in the market for a
new machine a bit sooner than i intended to be...

i'm looking into picking up a powerbook 160 or maybe 180 and have a bunch
of questions that (hopefully) somebody can answer:

* does anybody know any dirt on when the next round of powerbook
introductions are expected?  i'd heard the 185c was supposed to make an
appearence "this summer" but haven't heard anymore on it - and since i
don't have access to macleak, i was wondering if anybody out there had
more info...

* has anybody heard rumors about price drops to the powerbook line like the
ones the duo's just went through recently?

* what's the impression of the display on the 180?  i could probably swing
a 180 if i got the 80Mb disk rather than the 120, but i don't really have
a feel for how much "better" the display is (yea, it looks great in the
store, but is that all "wow" or is it really that good?).  could i solicit
some opinions of people who use the 160 and 180 day-to-day on if its worth
taking the disk size and money hit to get the active display?  (i realize
this is a real subjective question, but i've only played around with the
machines in a computer store breifly and figured the opinions of somebody
who actually uses the machine daily might prove helpful).

* how well does hellcats perform?  ;)

thanks a bunch in advance for any info - if you could email, i'll post a
summary (news reading time is at a premium with finals just around the
corner... :( )
--
Tom Willis     twillis@ecn.purdue.edu         Purdue Electrical Engineering</code></pre>
</div>
</div>
</section>
<section id="tokenization" class="slide level2">
<h2>Tokenization</h2>
<p>NLP requires converting natural language documents to numeric representations and performing computations on these representations. The first step to <strong>encoding</strong> documents into numeric representations is breaking the documents down into smaller units via <strong>tokenization</strong>. One obvious method of tokenization is <strong>word tokenization</strong>:</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a><span class="im">import</span> nltk</span>
<span id="cb6-2"><a href="#cb6-2"></a></span>
<span id="cb6-3"><a href="#cb6-3"></a>example <span class="op">=</span> d_train[<span class="st">"data"</span>][<span class="dv">0</span>]</span>
<span id="cb6-4"><a href="#cb6-4"></a></span>
<span id="cb6-5"><a href="#cb6-5"></a><span class="kw">def</span> word_tokenize(text):</span>
<span id="cb6-6"><a href="#cb6-6"></a>    x <span class="op">=</span> nltk.word_tokenize(text.replace(<span class="st">"</span><span class="ch">\\</span><span class="st">"</span>,<span class="st">" "</span>))</span>
<span id="cb6-7"><a href="#cb6-7"></a>    <span class="cf">return</span> x</span>
<span id="cb6-8"><a href="#cb6-8"></a></span>
<span id="cb6-9"><a href="#cb6-9"></a>example_tokenized <span class="op">=</span> word_tokenize(example)</span>
<span id="cb6-10"><a href="#cb6-10"></a></span>
<span id="cb6-11"><a href="#cb6-11"></a><span class="bu">print</span>(example <span class="op">+</span> <span class="st">"</span><span class="ch">\n</span><span class="st"> ==&gt; "</span>)</span>
<span id="cb6-12"><a href="#cb6-12"></a><span class="bu">print</span>(<span class="st">", "</span>.join([<span class="ss">f"'</span><span class="sc">{</span>t<span class="sc">}</span><span class="ss">'"</span> <span class="cf">for</span> t <span class="kw">in</span> example_tokenized]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>I was wondering if anyone out there could enlighten me on this car I saw
the other day. It was a 2-door sports car, looked to be from the late 60s/
early 70s. It was called a Bricklin. The doors were really small. In addition,
the front bumper was separate from the rest of the body. This is 
all I know. If anyone can tellme a model name, engine specs, years
of production, where this car is made, history, or whatever info you
have on this funky looking car, please e-mail.
 ==&gt; 
'I', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'I', 'saw', 'the', 'other', 'day', '.', 'It', 'was', 'a', '2-door', 'sports', 'car', ',', 'looked', 'to', 'be', 'from', 'the', 'late', '60s/', 'early', '70s', '.', 'It', 'was', 'called', 'a', 'Bricklin', '.', 'The', 'doors', 'were', 'really', 'small', '.', 'In', 'addition', ',', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', '.', 'This', 'is', 'all', 'I', 'know', '.', 'If', 'anyone', 'can', 'tellme', 'a', 'model', 'name', ',', 'engine', 'specs', ',', 'years', 'of', 'production', ',', 'where', 'this', 'car', 'is', 'made', ',', 'history', ',', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', ',', 'please', 'e-mail', '.'</code></pre>
</div>
</div>
</section>
<section id="vocabulary" class="slide level2">
<h2>Vocabulary</h2>
<p>We then create a <strong>vocabulary</strong> that maps tokens to indices:</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a><span class="im">from</span> itertools <span class="im">import</span> islice</span>
<span id="cb8-2"><a href="#cb8-2"></a><span class="bu">dict</span>(islice({j:i <span class="cf">for</span> i,j <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">set</span>(example_tokenized))}.items(),<span class="dv">10</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>{'know': 0,
 'be': 1,
 'production': 2,
 'were': 3,
 'enlighten': 4,
 'this': 5,
 'saw': 6,
 'late': 7,
 'all': 8,
 'tellme': 9}</code></pre>
</div>
</div>
</section>
<section id="building-a-vocabulary-from-a-corpus" class="slide level2">
<h2>Building a vocabulary from a corpus</h2>
<p>Of course we do not use just a single document to create our vocabulary, but a collection of documents, called a <strong>corpus</strong>. We also need to specify a maximum number of tokens for our vocabulary.</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a><span class="im">from</span> sklearn.feature_extraction.text <span class="im">import</span> CountVectorizer</span>
<span id="cb10-2"><a href="#cb10-2"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb10-3"><a href="#cb10-3"></a></span>
<span id="cb10-4"><a href="#cb10-4"></a>count_vectorizer <span class="op">=</span> CountVectorizer(</span>
<span id="cb10-5"><a href="#cb10-5"></a>    tokenizer <span class="op">=</span> word_tokenize,</span>
<span id="cb10-6"><a href="#cb10-6"></a>    max_features <span class="op">=</span> <span class="dv">1000</span>, <span class="co"># max number of tokens in our vocabulary</span></span>
<span id="cb10-7"><a href="#cb10-7"></a>    lowercase <span class="op">=</span> <span class="va">False</span>)</span>
<span id="cb10-8"><a href="#cb10-8"></a></span>
<span id="cb10-9"><a href="#cb10-9"></a><span class="co"># Train the vocabulary on 1000 example texts</span></span>
<span id="cb10-10"><a href="#cb10-10"></a>count_vectorizer.fit(d_train[<span class="st">"data"</span>][:<span class="dv">1000</span>])</span>
<span id="cb10-11"><a href="#cb10-11"></a></span>
<span id="cb10-12"><a href="#cb10-12"></a><span class="bu">print</span>(<span class="st">"Here's a subset of our vocabulary:"</span>)</span>
<span id="cb10-13"><a href="#cb10-13"></a><span class="bu">dict</span>(islice(count_vectorizer.vocabulary_.items(),<span class="dv">10</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Here's a subset of our vocabulary:</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>{'I': 146,
 'was': 948,
 'wondering': 979,
 'if': 572,
 'anyone': 313,
 'out': 731,
 'there': 893,
 'could': 414,
 'me': 659,
 'on': 718}</code></pre>
</div>
</div>
</section>
<section id="classical-nlp" class="slide level2">
<h2>Classical NLP</h2>
<p>Many traditional statistical and machine learning models expect data to be in a tabular format where columns correspond to specific features and rows correspond to individual observations (in the case of NLP, each document is treated as an individual observation).</p>
<p>A common method of transforming a corpus of documents into a tabular format is <strong>bag-of-words</strong> where each column in the table represents a given word (token) and the entry in row i, column j is the count of times word j occurs in document i.</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a>pd.DataFrame(</span>
<span id="cb13-2"><a href="#cb13-2"></a>    count_vectorizer.transform(d_train[<span class="st">"data"</span>][:<span class="dv">1000</span>]).toarray(),</span>
<span id="cb13-3"><a href="#cb13-3"></a>    columns <span class="op">=</span> count_vectorizer.get_feature_names_out().tolist())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>!</th>
      <th>#</th>
      <th>$</th>
      <th>%</th>
      <th>&amp;</th>
      <th>'</th>
      <th>''</th>
      <th>'AS</th>
      <th>'AX</th>
      <th>'d</th>
      <th>...</th>
      <th>x-Soviet</th>
      <th>year</th>
      <th>years</th>
      <th>yes</th>
      <th>yet</th>
      <th>you</th>
      <th>your</th>
      <th>{</th>
      <th>|</th>
      <th>}</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>995</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>996</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>3</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>997</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>998</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>999</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>3</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>1000 rows × 1000 columns</p>
</div>
</div>
</div>
</section>
<section id="improved-token-set" class="slide level2">
<h2>Improved token set</h2>
<p>The simple word level tokenization above produces a number of tokens that are unlikely to be informative. Given a fixed maximum number of tokens we can generally produce a more informative set of tokens by:</p>
<ol type="1">
<li>Lowercasing: no reason to produce columns for both “Apple” and “apple”</li>
<li>Removing <strong>stopwords</strong>, a list of common tokens such as “the”, “a”, “this”, etc. that are unlikely to add value in the bag of words approach.</li>
<li>Reducing morphological inflections:
<ul>
<li><strong>Stemming</strong> uses heuristic rules to reduce morphological inflections by chopping common suffixes from tokens.</li>
<li><strong>Lemmatization</strong> is a more sophisticated technique that generally uses vocabularies, word context and part-of-speech tagging to infer the correct lemma of a word. Lemmatization tends to be more computationally intensive than stemming.</li>
</ul></li>
</ol>
</section>
<section id="stopwords" class="slide level2">
<h2>Stopwords</h2>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1"></a><span class="im">import</span> sklearn</span>
<span id="cb14-2"><a href="#cb14-2"></a></span>
<span id="cb14-3"><a href="#cb14-3"></a>ENGLISH_STOP_WORDS <span class="op">=</span> sklearn.feature_extraction._stop_words.ENGLISH_STOP_WORDS</span>
<span id="cb14-4"><a href="#cb14-4"></a></span>
<span id="cb14-5"><a href="#cb14-5"></a><span class="bu">print</span>(<span class="st">"Here are the stopwords we'll remove:</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb14-6"><a href="#cb14-6"></a><span class="bu">print</span>(<span class="st">", "</span>.join(ENGLISH_STOP_WORDS))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Here are the stopwords we'll remove:

each, indeed, back, without, mostly, sincere, thereby, fifty, eleven, due, wherever, should, go, his, everyone, another, at, through, well, fill, former, all, that, ltd, nothing, part, twenty, becomes, five, formerly, put, became, only, sometimes, themselves, be, several, then, hasnt, else, cannot, noone, there, amongst, along, someone, beyond, against, an, those, thereupon, except, four, am, out, over, ever, neither, nevertheless, together, hereafter, thick, where, although, whereupon, them, among, whole, until, anyone, otherwise, least, please, herein, since, forty, do, found, her, call, whereas, become, somehow, however, mill, every, give, been, others, once, bill, twelve, throughout, ten, behind, from, mine, have, therefore, whoever, hereupon, might, further, here, across, anyhow, often, thin, also, whom, alone, seeming, most, rather, beside, less, thus, below, everything, onto, under, hers, keep, why, more, nowhere, moreover, anything, whose, being, wherein, on, one, fifteen, may, name, see, both, thereafter, their, still, re, are, again, or, per, could, six, many, upon, seem, was, even, amount, this, while, after, whether, anyway, how, into, whereafter, such, never, via, top, move, as, the, last, ie, him, latterly, must, none, enough, because, ours, get, always, about, whatever, system, would, had, hereby, your, before, these, elsewhere, everywhere, nor, up, by, afterwards, in, during, my, un, for, already, thence, us, serious, our, something, you, de, a, nobody, within, con, will, same, between, myself, much, not, made, which, is, take, can, either, few, its, inc, becoming, full, third, around, show, other, describe, if, meanwhile, me, two, who, when, eight, they, though, yourself, anywhere, now, himself, ourselves, whence, yours, latter, co, detail, above, some, sometime, whereby, has, whither, of, toward, so, whenever, seems, done, with, than, down, off, beforehand, very, i, perhaps, therein, thru, somewhere, own, he, fire, eg, sixty, any, hundred, bottom, she, almost, yet, we, find, no, towards, yourselves, first, hence, herself, etc, and, next, it, interest, were, besides, empty, seemed, too, cant, nine, what, cry, amoungst, couldnt, three, namely, side, but, front, itself, to</code></pre>
</div>
</div>
</section>
<section id="stemming" class="slide level2">
<h2>Stemming</h2>
<p>We’ll use the Porter stemmer for word stemming:</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1"></a><span class="im">from</span> nltk.stem.porter <span class="im">import</span> PorterStemmer</span>
<span id="cb16-2"><a href="#cb16-2"></a>porter_stemmer <span class="op">=</span> PorterStemmer()</span>
<span id="cb16-3"><a href="#cb16-3"></a></span>
<span id="cb16-4"><a href="#cb16-4"></a>stemming_example_words <span class="op">=</span> [<span class="st">"stop"</span>,<span class="st">"stops"</span>,<span class="st">"stopping"</span>,<span class="st">"stopped"</span>]</span>
<span id="cb16-5"><a href="#cb16-5"></a></span>
<span id="cb16-6"><a href="#cb16-6"></a><span class="cf">for</span> word <span class="kw">in</span> stemming_example_words:</span>
<span id="cb16-7"><a href="#cb16-7"></a>    <span class="bu">print</span>(<span class="ss">f"The stem of '</span><span class="sc">{</span>word<span class="sc">}</span><span class="ss">' is '</span><span class="sc">{</span>porter_stemmer<span class="sc">.</span>stem(word)<span class="sc">}</span><span class="ss">'"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The stem of 'stop' is 'stop'
The stem of 'stops' is 'stop'
The stem of 'stopping' is 'stop'
The stem of 'stopped' is 'stop'</code></pre>
</div>
</div>
</section>
<section id="final-tokenizer-and-vocabulary" class="slide level2">
<h2>Final tokenizer and vocabulary</h2>
<div class="cell" data-slideshow="{&quot;slide_type&quot;:&quot;-&quot;}" data-execution_count="11">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1"></a><span class="im">import</span> re</span>
<span id="cb18-2"><a href="#cb18-2"></a><span class="im">import</span> sklearn</span>
<span id="cb18-3"><a href="#cb18-3"></a></span>
<span id="cb18-4"><a href="#cb18-4"></a><span class="kw">def</span> word_tokenize(text, stopwords <span class="op">=</span> ENGLISH_STOP_WORDS):</span>
<span id="cb18-5"><a href="#cb18-5"></a>    <span class="co">"""Tokenize a string by:</span></span>
<span id="cb18-6"><a href="#cb18-6"></a><span class="co">    1. Tokenize words</span></span>
<span id="cb18-7"><a href="#cb18-7"></a><span class="co">    2. Filtering out tokens that don't contain at least</span></span>
<span id="cb18-8"><a href="#cb18-8"></a><span class="co">       two consecutive alpha-numeric characters or start with</span></span>
<span id="cb18-9"><a href="#cb18-9"></a><span class="co">       and apostrophe.</span></span>
<span id="cb18-10"><a href="#cb18-10"></a><span class="co">    3. Lowercase characters</span></span>
<span id="cb18-11"><a href="#cb18-11"></a><span class="co">    4. Remove stopwords.</span></span>
<span id="cb18-12"><a href="#cb18-12"></a><span class="co">    5. Remove repeated words.</span></span>
<span id="cb18-13"><a href="#cb18-13"></a><span class="co">    4. Apply the Porter stemmer</span></span>
<span id="cb18-14"><a href="#cb18-14"></a><span class="co">    """</span></span>
<span id="cb18-15"><a href="#cb18-15"></a>    x <span class="op">=</span> nltk.word_tokenize(text.replace(<span class="st">"</span><span class="ch">\\</span><span class="st">"</span>,<span class="st">" "</span>))</span>
<span id="cb18-16"><a href="#cb18-16"></a>    x <span class="op">=</span> [t <span class="cf">for</span> t <span class="kw">in</span> x <span class="cf">if</span> re.search(<span class="st">"[A-Za-z0-9]{2,}"</span>,t) <span class="kw">and</span> <span class="kw">not</span> re.match(<span class="st">"'.*"</span>,t)]</span>
<span id="cb18-17"><a href="#cb18-17"></a>    x <span class="op">=</span> [t.lower() <span class="cf">for</span> t <span class="kw">in</span> x] <span class="co"># lowercase</span></span>
<span id="cb18-18"><a href="#cb18-18"></a>    x <span class="op">=</span> [t <span class="cf">for</span> t <span class="kw">in</span> x <span class="cf">if</span> t <span class="kw">not</span> <span class="kw">in</span> stopwords]</span>
<span id="cb18-19"><a href="#cb18-19"></a>    x <span class="op">=</span> [x[i] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(x)) <span class="cf">if</span> i<span class="op">==</span><span class="dv">0</span> <span class="kw">or</span> x[i] <span class="op">!=</span> x[i<span class="op">-</span><span class="dv">1</span>]]</span>
<span id="cb18-20"><a href="#cb18-20"></a>    <span class="cf">return</span> [porter_stemmer.stem(t) <span class="cf">for</span> t <span class="kw">in</span> x]</span>
<span id="cb18-21"><a href="#cb18-21"></a></span>
<span id="cb18-22"><a href="#cb18-22"></a>count_vectorizer <span class="op">=</span> CountVectorizer(</span>
<span id="cb18-23"><a href="#cb18-23"></a>    <span class="co">#tokenizer=word_tokenize,</span></span>
<span id="cb18-24"><a href="#cb18-24"></a>    analyzer<span class="op">=</span>word_tokenize,</span>
<span id="cb18-25"><a href="#cb18-25"></a>    max_features<span class="op">=</span><span class="dv">100</span></span>
<span id="cb18-26"><a href="#cb18-26"></a>    )</span>
<span id="cb18-27"><a href="#cb18-27"></a></span>
<span id="cb18-28"><a href="#cb18-28"></a>count_vectorizer.fit(d_train[<span class="st">"data"</span>][:<span class="dv">1000</span>])</span>
<span id="cb18-29"><a href="#cb18-29"></a></span>
<span id="cb18-30"><a href="#cb18-30"></a>pd.DataFrame(count_vectorizer.transform(d_train[<span class="st">"data"</span>][:<span class="dv">1000</span>]).toarray(),</span>
<span id="cb18-31"><a href="#cb18-31"></a>columns <span class="op">=</span> count_vectorizer.get_feature_names_out().tolist())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>applic</th>
      <th>argument</th>
      <th>armenian</th>
      <th>ask</th>
      <th>believ</th>
      <th>better</th>
      <th>ca</th>
      <th>call</th>
      <th>car</th>
      <th>case</th>
      <th>...</th>
      <th>tri</th>
      <th>true</th>
      <th>understand</th>
      <th>use</th>
      <th>want</th>
      <th>way</th>
      <th>window</th>
      <th>word</th>
      <th>work</th>
      <th>year</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>4</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>995</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>996</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>997</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>998</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>999</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>7</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>1000 rows × 100 columns</p>
</div>
</div>
</div>
</section>
<section id="n-grams" class="slide level2">
<h2>N-grams</h2>
<p>One of the primary downsides of bag-of-words is that it loses all information contained in the order of the tokens. For example, both “Dog bites man” and “Man bites dog” produce the same bag-of-words representation. One method for partially overcoming this shortcoming is to use <strong>n-grams</strong>; the creation of a new token by combining <code>n</code> consecutive tokens</p>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1"></a><span class="im">import</span> random</span>
<span id="cb19-2"><a href="#cb19-2"></a>random.seed(<span class="dv">1</span>)</span>
<span id="cb19-3"><a href="#cb19-3"></a></span>
<span id="cb19-4"><a href="#cb19-4"></a>count_vectorizer <span class="op">=</span> CountVectorizer(</span>
<span id="cb19-5"><a href="#cb19-5"></a>    tokenizer<span class="op">=</span>word_tokenize,</span>
<span id="cb19-6"><a href="#cb19-6"></a>    max_features<span class="op">=</span><span class="dv">2000</span>,</span>
<span id="cb19-7"><a href="#cb19-7"></a>    ngram_range<span class="op">=</span>(<span class="dv">1</span>,<span class="dv">4</span>), <span class="co"># n-grams from 1-4</span></span>
<span id="cb19-8"><a href="#cb19-8"></a>    max_df <span class="op">=</span> <span class="fl">0.5</span>, <span class="co"># tokens occurring in a higher proportion of documents than this are left out</span></span>
<span id="cb19-9"><a href="#cb19-9"></a>    min_df <span class="op">=</span> <span class="fl">0.0001</span> <span class="co"># tokens occurring in a lower proportion of documents than this are left out</span></span>
<span id="cb19-10"><a href="#cb19-10"></a>    )</span>
<span id="cb19-11"><a href="#cb19-11"></a></span>
<span id="cb19-12"><a href="#cb19-12"></a>count_vectorizer.fit(d_train[<span class="st">"data"</span>])</span>
<span id="cb19-13"><a href="#cb19-13"></a></span>
<span id="cb19-14"><a href="#cb19-14"></a>random_sample_n_grams <span class="op">=</span> random.sample(</span>
<span id="cb19-15"><a href="#cb19-15"></a>    [i <span class="cf">for</span> i <span class="kw">in</span> count_vectorizer.get_feature_names_out() <span class="cf">if</span> re.search(<span class="st">" "</span>,i)],<span class="dv">10</span>)</span>
<span id="cb19-16"><a href="#cb19-16"></a></span>
<span id="cb19-17"><a href="#cb19-17"></a><span class="bu">print</span>(<span class="st">"Some example n-grams for n&gt;=2 </span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb19-18"><a href="#cb19-18"></a></span>
<span id="cb19-19"><a href="#cb19-19"></a><span class="bu">print</span>(<span class="st">", "</span>.join([<span class="ss">f"'</span><span class="sc">{</span>t<span class="sc">}</span><span class="ss">'"</span> <span class="cf">for</span> t <span class="kw">in</span> random_sample_n_grams]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Some example n-grams for n&gt;=2 

'doe know', 'serial number', 'avail anonym ftp', 'hard disk', 'disk drive', 'new york', 'make sure', 'ms. myer', 'long time', 'ftp site'</code></pre>
</div>
</div>
</section>
<section id="text-classification-with-logistic-regression" class="slide level2">
<h2>Text Classification with Logistic Regression</h2>
<p>Finally we can use our matrix of vectorized texts to fit a logistic regression model for topic classification. We pipe the output of our <code>CountVectorizer</code> through a <code>TfidfTransformer</code> (Term Frequency Inverse Document Frequency) object that normalizes document-level token counts by corpus-level token frequency. This is then piped into a logistic regression classifier. We fit our pipeline on the training data.</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> Pipeline</span>
<span id="cb21-2"><a href="#cb21-2"></a><span class="im">from</span> sklearn.feature_extraction.text <span class="im">import</span> TfidfTransformer</span>
<span id="cb21-3"><a href="#cb21-3"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb21-4"><a href="#cb21-4"></a></span>
<span id="cb21-5"><a href="#cb21-5"></a>pipe <span class="op">=</span> Pipeline([</span>
<span id="cb21-6"><a href="#cb21-6"></a>    (<span class="st">"count_vectorizer"</span>, count_vectorizer),</span>
<span id="cb21-7"><a href="#cb21-7"></a>    (<span class="st">"tfidf_transformer"</span>, TfidfTransformer()),</span>
<span id="cb21-8"><a href="#cb21-8"></a>    (<span class="st">"logistic_reg"</span>, LogisticRegression(multi_class<span class="op">=</span><span class="st">'multinomial'</span>))])</span>
<span id="cb21-9"><a href="#cb21-9"></a></span>
<span id="cb21-10"><a href="#cb21-10"></a>pipe.fit(d_train[<span class="st">"data"</span>],d_train[<span class="st">"target"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>Pipeline(steps=[('count_vectorizer',
                 CountVectorizer(max_df=0.5, max_features=2000, min_df=0.0001,
                                 ngram_range=(1, 4),
                                 tokenizer=&lt;function word_tokenize at 0x0000014B317B5940&gt;)),
                ('tfidf_transformer', TfidfTransformer()),
                ('logistic_reg',
                 LogisticRegression(multi_class='multinomial'))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox"><label for="sk-estimator-id-1" class="sk-toggleable__label sk-toggleable__label-arrow">Pipeline</label><div class="sk-toggleable__content"><pre>Pipeline(steps=[('count_vectorizer',
                 CountVectorizer(max_df=0.5, max_features=2000, min_df=0.0001,
                                 ngram_range=(1, 4),
                                 tokenizer=&lt;function word_tokenize at 0x0000014B317B5940&gt;)),
                ('tfidf_transformer', TfidfTransformer()),
                ('logistic_reg',
                 LogisticRegression(multi_class='multinomial'))])</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-2" type="checkbox"><label for="sk-estimator-id-2" class="sk-toggleable__label sk-toggleable__label-arrow">CountVectorizer</label><div class="sk-toggleable__content"><pre>CountVectorizer(max_df=0.5, max_features=2000, min_df=0.0001,
                ngram_range=(1, 4),
                tokenizer=&lt;function word_tokenize at 0x0000014B317B5940&gt;)</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-3" type="checkbox"><label for="sk-estimator-id-3" class="sk-toggleable__label sk-toggleable__label-arrow">TfidfTransformer</label><div class="sk-toggleable__content"><pre>TfidfTransformer()</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-4" type="checkbox"><label for="sk-estimator-id-4" class="sk-toggleable__label sk-toggleable__label-arrow">LogisticRegression</label><div class="sk-toggleable__content"><pre>LogisticRegression(multi_class='multinomial')</pre></div></div></div></div></div></div></div>
</div>
</div>
<p>We evaluate the model using precision and recall. For a given topic:</p>
<ul>
<li><p><strong>Precision</strong> is the number of times we predicted that topic correctly divided by the number of times we predicted that topic (both correctly and incorrectly).</p></li>
<li><p><strong>Recall</strong> is the number of times we predicted that topic correctly divided by the number of actual cases of the topic.</p></li>
</ul>
<p>A model that just guessed randomly and uniformly on our dataset would have both precision and recall close to 0.05. Our simple model is already much better than that:</p>
<div class="cell" data-slideshow="{&quot;slide_type&quot;:&quot;-&quot;}" data-execution_count="14">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> classification_report</span>
<span id="cb22-2"><a href="#cb22-2"></a></span>
<span id="cb22-3"><a href="#cb22-3"></a>test_preds <span class="op">=</span> pipe.predict(d_test[<span class="st">"data"</span>])</span>
<span id="cb22-4"><a href="#cb22-4"></a></span>
<span id="cb22-5"><a href="#cb22-5"></a><span class="bu">print</span>(</span>
<span id="cb22-6"><a href="#cb22-6"></a>    <span class="ss">f"Classification report for classifier:</span><span class="ch">\n</span><span class="ss">"</span> <span class="op">+</span></span>
<span id="cb22-7"><a href="#cb22-7"></a>    <span class="ss">f"""</span><span class="sc">{</span>classification_report(</span>
<span id="cb22-8"><a href="#cb22-8"></a>        d_test[<span class="st">'target'</span>], test_preds,</span>
<span id="cb22-9"><a href="#cb22-9"></a>        target_names<span class="op">=</span>d_test[<span class="st">'target_names'</span>])<span class="sc">}</span><span class="ch">\n</span><span class="ss">"""</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Classification report for classifier:
                          precision    recall  f1-score   support

             alt.atheism       0.41      0.42      0.41       319
           comp.graphics       0.56      0.59      0.57       389
 comp.os.ms-windows.misc       0.57      0.55      0.56       394
comp.sys.ibm.pc.hardware       0.58      0.56      0.57       392
   comp.sys.mac.hardware       0.63      0.56      0.59       385
          comp.windows.x       0.70      0.61      0.65       395
            misc.forsale       0.71      0.74      0.72       390
               rec.autos       0.66      0.62      0.64       396
         rec.motorcycles       0.42      0.70      0.53       398
      rec.sport.baseball       0.72      0.75      0.73       397
        rec.sport.hockey       0.83      0.79      0.81       399
               sci.crypt       0.81      0.63      0.71       396
         sci.electronics       0.48      0.53      0.50       393
                 sci.med       0.68      0.68      0.68       396
               sci.space       0.69      0.64      0.67       394
  soc.religion.christian       0.60      0.73      0.66       398
      talk.politics.guns       0.51      0.58      0.54       364
   talk.politics.mideast       0.79      0.69      0.73       376
      talk.politics.misc       0.48      0.40      0.43       310
      talk.religion.misc       0.32      0.16      0.21       251

                accuracy                           0.61      7532
               macro avg       0.61      0.60      0.60      7532
            weighted avg       0.62      0.61      0.61      7532

</code></pre>
</div>
</div>
</section>
<section id="topic-modeling" class="slide level2">
<h2>Topic Modeling</h2>
<p>In many use cases, labeled training data is limited or not available. It is still possible though to learn about the documents in such a corpus. Topic modeling constructs latent topics from a corpus and represents each text as some combination of these latent topics.</p>
</section>
<section class="slide level2">

<div class="cell" data-slideshow="{&quot;slide_type&quot;:&quot;slide&quot;}" data-execution_count="15">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> LatentDirichletAllocation</span>
<span id="cb24-2"><a href="#cb24-2"></a></span>
<span id="cb24-3"><a href="#cb24-3"></a>n_components <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb24-4"><a href="#cb24-4"></a></span>
<span id="cb24-5"><a href="#cb24-5"></a>lda <span class="op">=</span> LatentDirichletAllocation(</span>
<span id="cb24-6"><a href="#cb24-6"></a>    n_components<span class="op">=</span>n_components,</span>
<span id="cb24-7"><a href="#cb24-7"></a>    <span class="co">#max_iter=5,</span></span>
<span id="cb24-8"><a href="#cb24-8"></a>    <span class="co">#learning_method="online",</span></span>
<span id="cb24-9"><a href="#cb24-9"></a>    <span class="co">#learning_offset=50.0,</span></span>
<span id="cb24-10"><a href="#cb24-10"></a>    random_state<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb24-11"><a href="#cb24-11"></a>)</span>
<span id="cb24-12"><a href="#cb24-12"></a></span>
<span id="cb24-13"><a href="#cb24-13"></a>lda_pipe <span class="op">=</span> Pipeline([</span>
<span id="cb24-14"><a href="#cb24-14"></a>    (<span class="st">"count_vectorizer"</span>, count_vectorizer),</span>
<span id="cb24-15"><a href="#cb24-15"></a>    (<span class="st">"tfidf_transformer"</span>, TfidfTransformer()),</span>
<span id="cb24-16"><a href="#cb24-16"></a>    (<span class="st">"lda"</span>, lda)])</span>
<span id="cb24-17"><a href="#cb24-17"></a></span>
<span id="cb24-18"><a href="#cb24-18"></a>lda_pipe.fit(d_train[<span class="st">"data"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="14">
<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-2" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>Pipeline(steps=[('count_vectorizer',
                 CountVectorizer(max_df=0.5, max_features=2000, min_df=0.0001,
                                 ngram_range=(1, 4),
                                 tokenizer=&lt;function word_tokenize at 0x0000014B317B5940&gt;)),
                ('tfidf_transformer', TfidfTransformer()),
                ('lda',
                 LatentDirichletAllocation(n_components=20, random_state=0))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-5" type="checkbox"><label for="sk-estimator-id-5" class="sk-toggleable__label sk-toggleable__label-arrow">Pipeline</label><div class="sk-toggleable__content"><pre>Pipeline(steps=[('count_vectorizer',
                 CountVectorizer(max_df=0.5, max_features=2000, min_df=0.0001,
                                 ngram_range=(1, 4),
                                 tokenizer=&lt;function word_tokenize at 0x0000014B317B5940&gt;)),
                ('tfidf_transformer', TfidfTransformer()),
                ('lda',
                 LatentDirichletAllocation(n_components=20, random_state=0))])</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-6" type="checkbox"><label for="sk-estimator-id-6" class="sk-toggleable__label sk-toggleable__label-arrow">CountVectorizer</label><div class="sk-toggleable__content"><pre>CountVectorizer(max_df=0.5, max_features=2000, min_df=0.0001,
                ngram_range=(1, 4),
                tokenizer=&lt;function word_tokenize at 0x0000014B317B5940&gt;)</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-7" type="checkbox"><label for="sk-estimator-id-7" class="sk-toggleable__label sk-toggleable__label-arrow">TfidfTransformer</label><div class="sk-toggleable__content"><pre>TfidfTransformer()</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-8" type="checkbox"><label for="sk-estimator-id-8" class="sk-toggleable__label sk-toggleable__label-arrow">LatentDirichletAllocation</label><div class="sk-toggleable__content"><pre>LatentDirichletAllocation(n_components=20, random_state=0)</pre></div></div></div></div></div></div></div>
</div>
</div>
</section>
<section class="slide level2">

<div class="cell" data-fig-height="10" data-fig-width="10" data-slideshow="{&quot;slide_type&quot;:&quot;slide&quot;}" data-execution_count="16">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1"></a><span class="im">from</span> utils <span class="im">import</span> plot_top_words</span>
<span id="cb25-2"><a href="#cb25-2"></a>n_top_words<span class="op">=</span><span class="dv">10</span></span>
<span id="cb25-3"><a href="#cb25-3"></a>tf_feature_names <span class="op">=</span> lda_pipe[<span class="st">"count_vectorizer"</span>].get_feature_names_out()</span>
<span id="cb25-4"><a href="#cb25-4"></a>fig <span class="op">=</span> plot_top_words(lda_pipe[<span class="st">"lda"</span>], tf_feature_names, n_top_words, <span class="st">"Topics in LDA model"</span>)</span>
<span id="cb25-5"><a href="#cb25-5"></a></span>
<span id="cb25-6"><a href="#cb25-6"></a><span class="co">#fig.tight_layout();</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="top-tokens-per-topic" class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="Intro%20to%20NLP_files/figure-revealjs/top-tokens-per-topic-output-1.png" width="1315" height="2911"></p>
<p></p><figcaption>Top token for each discovered topic</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
<section class="slide level2">

<div class="cell" data-slideshow="{&quot;slide_type&quot;:&quot;slide&quot;}" data-execution_count="17">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1"></a>lda_test_preds <span class="op">=</span> lda_pipe.transform(d_test[<span class="st">"data"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-slideshow="{&quot;slide_type&quot;:&quot;-&quot;}" data-execution_count="18">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb27-2"><a href="#cb27-2"></a><span class="im">from</span> matplotlib.colors <span class="im">import</span> ListedColormap, to_hex</span>
<span id="cb27-3"><a href="#cb27-3"></a>tab20 <span class="op">=</span> plt.get_cmap(<span class="st">"tab20"</span>)</span>
<span id="cb27-4"><a href="#cb27-4"></a>lcm <span class="op">=</span> ListedColormap(tab20(<span class="bu">range</span>(<span class="dv">20</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">15</span>,<span class="dv">10</span>))</span>
<span id="cb28-2"><a href="#cb28-2"></a><span class="cf">for</span> row <span class="kw">in</span> (pd.DataFrame(lda_test_preds)</span>
<span id="cb28-3"><a href="#cb28-3"></a>  .assign(target <span class="op">=</span> d_test[<span class="st">"target"</span>])</span>
<span id="cb28-4"><a href="#cb28-4"></a>  .sample(<span class="dv">100</span>, random_state<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb28-5"><a href="#cb28-5"></a>  .iterrows()):</span>
<span id="cb28-6"><a href="#cb28-6"></a>    ax.plot(<span class="bu">range</span>(n_components),row[<span class="dv">1</span>][:n_components]</span>
<span id="cb28-7"><a href="#cb28-7"></a>    <span class="co">#,c=row[1]["target"]</span></span>
<span id="cb28-8"><a href="#cb28-8"></a>    , c<span class="op">=</span>lcm(<span class="bu">int</span>(row[<span class="dv">1</span>][<span class="st">"target"</span>]))</span>
<span id="cb28-9"><a href="#cb28-9"></a>    )</span>
<span id="cb28-10"><a href="#cb28-10"></a><span class="im">from</span> matplotlib <span class="im">import</span> patches</span>
<span id="cb28-11"><a href="#cb28-11"></a>handles <span class="op">=</span> [patches.Patch(color <span class="op">=</span> lcm(i), label<span class="op">=</span>d_test[<span class="st">"target_names"</span>][i]) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">20</span>)]</span>
<span id="cb28-12"><a href="#cb28-12"></a>ax.legend(handles<span class="op">=</span>handles)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="18">
<pre><code>&lt;matplotlib.legend.Legend at 0x14b0b4dcee0&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img data-src="Intro%20to%20NLP_files/figure-revealjs/cell-19-output-2.png" width="1166" height="766"></p>
</div>
</div>
</section>
<section class="slide level2">

<div class="cell" data-slideshow="{&quot;slide_type&quot;:&quot;slide&quot;}" data-execution_count="20">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1"></a><span class="im">from</span> sklearn <span class="im">import</span> manifold</span>
<span id="cb30-2"><a href="#cb30-2"></a>tsne <span class="op">=</span> manifold.TSNE(</span>
<span id="cb30-3"><a href="#cb30-3"></a>        n_components<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb30-4"><a href="#cb30-4"></a>        init<span class="op">=</span><span class="st">"random"</span>,</span>
<span id="cb30-5"><a href="#cb30-5"></a>        random_state<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb30-6"><a href="#cb30-6"></a>        perplexity<span class="op">=</span><span class="dv">200</span>,</span>
<span id="cb30-7"><a href="#cb30-7"></a>        learning_rate<span class="op">=</span><span class="st">"auto"</span>,</span>
<span id="cb30-8"><a href="#cb30-8"></a>        n_iter<span class="op">=</span><span class="dv">300</span>,</span>
<span id="cb30-9"><a href="#cb30-9"></a>    )</span>
<span id="cb30-10"><a href="#cb30-10"></a>Y <span class="op">=</span> tsne.fit_transform(lda_test_preds)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1"></a>plt.scatter(Y[:,<span class="dv">0</span>],Y[:,<span class="dv">1</span>],</span>
<span id="cb31-2"><a href="#cb31-2"></a>c<span class="op">=</span>d_test[<span class="st">"target"</span>],</span>
<span id="cb31-3"><a href="#cb31-3"></a>cmap<span class="op">=</span><span class="st">"tab20"</span></span>
<span id="cb31-4"><a href="#cb31-4"></a><span class="co">#c=[d_test["target_names"][i] for i in d_test["target"]]</span></span>
<span id="cb31-5"><a href="#cb31-5"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="20">
<pre><code>&lt;matplotlib.collections.PathCollection at 0x14b497abf40&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img data-src="Intro%20to%20NLP_files/figure-revealjs/cell-21-output-2.png" width="801" height="766"></p>
</div>
</div>
</section>
<section id="state-of-the-art-nlp-with-transformers" class="slide level2">
<h2>State-of-the-Art NLP with Transformers</h2>
<p>All state-of-the-art models in NLP are now a type of deep neural network using a variant of the architecture called a “transformer”.</p>
<p>The transformer architecture was originally introduced in the paper <a href="https://arxiv.org/pdf/1706.03762.pdf">Attention is All You Need</a> by Vaswani et al.&nbsp;2017 (Google Brain/Google Research).</p>

<img data-src="images/transformer_architecture.png" class="r-stretch quarto-figure-center"><p class="caption">Transformer Architecture</p><p><em>Attention is All You Need</em> was concerned with translation, for instance from English to German or English to French. In the case of English to German, the inputs to the Transformer Encoder are the full sequence of English token indices from a document, <span class="math inline">\(\mathbb{x} = (x_1, x_2, ... , x_n)\)</span>. The output of the Transformer Encoder are a corresponding sequence of vectors <span class="math inline">\(\mathbb{z} = (z_1, z_2, ... , z_n)\)</span>.</p>
<p>The Transformer Decoder then generates an output sequence <span class="math inline">\(\mathbb{y} = (y_1, y_2, ... , y_m)\)</span> of German token indices one at a time. At each time step the Transformer Decoder takes the Transformer Encoder output <span class="math inline">\(\mathbb{z}\)</span> and the part of <span class="math inline">\(\mathbb{y}\)</span> that it had previously generated (<span class="math inline">\((y_1, ..., y_k)\)</span> where <span class="math inline">\(k&lt;m\)</span>) and produces the next token <span class="math inline">\(y_{k+1}\)</span></p>
<p>The Transformer was trained on several million English-German (or English-French) sentence pairs using backpropogation.</p>
</section>
<section id="transformers-take-over-all-of-nlp" class="slide level2">
<h2>Transformers take over all of NLP</h2>
<p>Since the publication of <em>Attention is All You Need</em> transformers have pushed out the performance frontier in all NLP tasks. There are three main branches of the transformer family tree:</p>
<ul>
<li>Full transformer: This is the original transformer from <em>Attention is All You Need</em>. These models excel at tasks that make use of a reference text and generate a new output text, for instance translation and abstractive summarization.</li>
<li>Decoder-only transformer: these are good at generating text without a provided reference text. Decoder-only models excel as chat bots and writing stories given a prompt.</li>
<li>Encoder-only transformer: these excel in any NLP task that does not require generating new text, for instance classification, named entity recognition and extractive question answering and summarization.</li>
</ul>
</section>
<section id="decoder-only-transformers---gpt" class="slide level2">
<h2>Decoder-only Transformers - GPT</h2>
<p>In <a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">Improving Language Understanding by Generative Pre-Training</a>, Radford et al 2018 (OpenAI), used just the Transformer Decoder and introduced a novel two-stage training paradigm:</p>
<p><strong>Pretraining (Unsupervised)</strong>: Train the model on a large corpus of unlabeled text data (typically scraped from the web). Given a chunk of text as input, the model’s objective is to predict the word that came next in the source text. For instance, the model receives the input <code>["The", "cat", "is", "on", "the"]</code> and must predict which word came next. Pretraining allows the model to learn general features of the language.</p>
<p><strong>Fine-tuning (Supervised)</strong>: For a specific task with limited labeled training data, like classification or textual entailment, initialize a similar model architecture using the weights learned from pretraining and continue training for the specific task. This requires some creativity in formatting the input data to the model and attaching an additional output layer that is task-specific.</p>

<img data-src="images/gpt_objectives.png" class="r-stretch quarto-figure-center"><p class="caption">GPT Pretraining and Fine-tuning</p><p>Transformer Decoder models excel at tasks requiring text generation, for instance chat bots. The most cited transformer decoder models include:</p>
<ul>
<li>OpenAI’s GPT, GPT-2, GPT-3 and GPT-Neo</li>
<li>Google’s PaLM and LaMDA</li>
</ul>
</section>
<section id="encoder-only-transformers---bert" class="slide level2">
<h2>Encoder-only transformers - BERT</h2>
<p>Even though many languages are written and read left to right, the meaning of words and concepts in sentences flow in both directions. The Transformer Decoder-only models do not take advantage of this bidirectionality. Consider the sentence: “The bat that flew through the night air almost hit the player on deck.” If you only saw the first 8 words (“The bat that flew through the night air”) you probably have a very different understanding of “bat” than if you saw the full sentence. The correct meaning of the word “bat” in this sentence flows backwords from “the player on deck”.</p>
<p>In <a href="https://arxiv.org/pdf/1810.04805.pdf">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a> (Devlin et al.&nbsp;2019 - also at Google) took just the Transformer Encoder from <em>Attention is All You Need</em> and extended the pretraining/fine-tuning approach to produced models that take advantage of bidirectionality and that achieved state-of-the-art performance.</p>
<p><strong>Pretraining (Unsupervised)</strong>: Just like in Radford et al., train the model on a large corpus of unlabeled text data. Given a chunk of text as input, the model’s objective is to predict randomly masked words. For instance, the model receives the input <code>["The", "[MASK]", "is", "on", "[MASK]", "mat"]</code> and must predict which words are under the <code>"[MASK]"</code> tokens. The original BERT paper also included another pretraining task, next sentence prediction, in which the model had to predict whether one sentence followed another in a source text. This task is generally dropped in more recent versions of BERT.</p>
<p><strong>Fine-tuning (Supervised)</strong>: The fine-tuning stage is similar to that in Radford et al.: for each task, initialize a modified architecture with weights learned from pretraining and continue to train the model on labeled data.</p>
</section>
<section id="using-googles-pegasus-full-transformer-model-for-abstractive-summarization." class="slide level2">
<h2>Using Google’s PEGASUS full transformer model for abstractive summarization.</h2>
<p>Now we’ll do an actual demo of <a href="https://arxiv.org/pdf/1912.08777.pdf">Google’s PEGASUS</a> full transformer model to perform abstractive summarization.</p>
<p>Abstractive summarization takes a reference text as input to the transformer encoder and generates a summary using the transformer decoder.</p>
<p>Google provides a trained PEGASUS model in the <a href="https://huggingface.co/models">Huggingface Model Hub</a>. <code>Huggingface</code> is an opensource project that provides datasets, pretrained models and software for creating, training and using models across <code>torch</code>, <code>tensorflow</code> and <code>JAX</code>.</p>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline, set_seed</span>
<span id="cb33-2"><a href="#cb33-2"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset, load_metric</span>
<span id="cb33-3"><a href="#cb33-3"></a><span class="im">import</span> nltk</span>
<span id="cb33-4"><a href="#cb33-4"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb33-5"><a href="#cb33-5"></a><span class="im">from</span> nltk.tokenize <span class="im">import</span> sent_tokenize</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="cnndaily-mail-dataset" class="slide level2">
<h2>CNN/Daily Mail Dataset</h2>
<p>We’ll download the CNN/Daily Mail Dataset from Huggingface. The CNN/Daily Mail dataset is well-suited for abstractive summarization because CNN/Daily Mail themselves provide brief summaries of their articles.</p>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1"></a>dataset <span class="op">=</span> load_dataset(<span class="st">"cnn_dailymail"</span>, version<span class="op">=</span><span class="st">"3.0.0"</span>)</span>
<span id="cb34-2"><a href="#cb34-2"></a><span class="bu">print</span>(<span class="ss">f"Features: </span><span class="sc">{</span>dataset[<span class="st">'train'</span>]<span class="sc">.</span>column_names<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"f2b3fff2b9f24213b32f395bcce25090","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Features: ['article', 'highlights', 'id']</code></pre>
</div>
</div>
<p>Here’s an example of an article and its summary:</p>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1"></a>sample_idx <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb36-2"><a href="#cb36-2"></a>sample <span class="op">=</span> dataset[<span class="st">"test"</span>][sample_idx]</span>
<span id="cb36-3"><a href="#cb36-3"></a><span class="bu">print</span>(<span class="ss">f"""</span></span>
<span id="cb36-4"><a href="#cb36-4"></a><span class="ss">Article (total length: </span><span class="sc">{</span><span class="bu">len</span>(sample[<span class="st">"article"</span>])<span class="sc">}</span><span class="ss">):</span></span>
<span id="cb36-5"><a href="#cb36-5"></a><span class="ss">"""</span>)</span>
<span id="cb36-6"><a href="#cb36-6"></a><span class="bu">print</span>(sample[<span class="st">"article"</span>])</span>
<span id="cb36-7"><a href="#cb36-7"></a><span class="bu">print</span>(<span class="ss">f'</span><span class="ch">\n</span><span class="ss">Summary (length: </span><span class="sc">{</span><span class="bu">len</span>(sample[<span class="st">"highlights"</span>])<span class="sc">}</span><span class="ss">):'</span>)</span>
<span id="cb36-8"><a href="#cb36-8"></a><span class="bu">print</span>(sample[<span class="st">"highlights"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Article (total length: 3612):

(CNN)The Palestinian Authority officially became the 123rd member of the International Criminal Court on Wednesday, a step that gives the court jurisdiction over alleged crimes in Palestinian territories. The formal accession was marked with a ceremony at The Hague, in the Netherlands, where the court is based. The Palestinians signed the ICC's founding Rome Statute in January, when they also accepted its jurisdiction over alleged crimes committed "in the occupied Palestinian territory, including East Jerusalem, since June 13, 2014." Later that month, the ICC opened a preliminary examination into the situation in Palestinian territories, paving the way for possible war crimes investigations against Israelis. As members of the court, Palestinians may be subject to counter-charges as well. Israel and the United States, neither of which is an ICC member, opposed the Palestinians' efforts to join the body. But Palestinian Foreign Minister Riad al-Malki, speaking at Wednesday's ceremony, said it was a move toward greater justice. "As Palestine formally becomes a State Party to the Rome Statute today, the world is also a step closer to ending a long era of impunity and injustice," he said, according to an ICC news release. "Indeed, today brings us closer to our shared goals of justice and peace." Judge Kuniko Ozaki, a vice president of the ICC, said acceding to the treaty was just the first step for the Palestinians. "As the Rome Statute today enters into force for the State of Palestine, Palestine acquires all the rights as well as responsibilities that come with being a State Party to the Statute. These are substantive commitments, which cannot be taken lightly," she said. Rights group Human Rights Watch welcomed the development. "Governments seeking to penalize Palestine for joining the ICC should immediately end their pressure, and countries that support universal acceptance of the court's treaty should speak out to welcome its membership," said Balkees Jarrah, international justice counsel for the group. "What's objectionable is the attempts to undermine international justice, not Palestine's decision to join a treaty to which over 100 countries around the world are members." In January, when the preliminary ICC examination was opened, Israeli Prime Minister Benjamin Netanyahu described it as an outrage, saying the court was overstepping its boundaries. The United States also said it "strongly" disagreed with the court's decision. "As we have said repeatedly, we do not believe that Palestine is a state and therefore we do not believe that it is eligible to join the ICC," the State Department said in a statement. It urged the warring sides to resolve their differences through direct negotiations. "We will continue to oppose actions against Israel at the ICC as counterproductive to the cause of peace," it said. But the ICC begs to differ with the definition of a state for its purposes and refers to the territories as "Palestine." While a preliminary examination is not a formal investigation, it allows the court to review evidence and determine whether to investigate suspects on both sides. Prosecutor Fatou Bensouda said her office would "conduct its analysis in full independence and impartiality." The war between Israel and Hamas militants in Gaza last summer left more than 2,000 people dead. The inquiry will include alleged war crimes committed since June. The International Criminal Court was set up in 2002 to prosecute genocide, crimes against humanity and war crimes. CNN's Vasco Cotovio, Kareem Khadder and Faith Karimi contributed to this report.

Summary (length: 233):
Membership gives the ICC jurisdiction over alleged crimes committed in Palestinian territories since last June .
Israel and the United States opposed the move, which could open the door to war crimes investigations against Israelis .</code></pre>
</div>
</div>
</section>
<section class="slide level2">

<div class="cell" data-slideshow="{&quot;slide_type&quot;:&quot;slide&quot;}" data-execution_count="25">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1"></a><span class="co"># Download Google's PEGASUS model that was fine-tuned on CNN/Daily Mail</span></span>
<span id="cb38-2"><a href="#cb38-2"></a></span>
<span id="cb38-3"><a href="#cb38-3"></a>pipe <span class="op">=</span> pipeline(<span class="st">"summarization"</span>, model<span class="op">=</span><span class="st">"google/pegasus-cnn_dailymail"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<h3 id="digression-on-subword-tokenization">Digression on subword tokenization</h3>
<p>Modern transformer models typically use a form of tokenization called subword tokenization rather than the word tokenization that we used before. This allows the model to construct rare words or words that did not occur in the training data from subword pieces rather than treating them as out-of-vocabulary:</p>
<div class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1"></a>encoded <span class="op">=</span> pipe.tokenizer.encode(<span class="st">"""</span></span>
<span id="cb39-2"><a href="#cb39-2"></a><span class="st">Hello. My name is Greg Strabel</span></span>
<span id="cb39-3"><a href="#cb39-3"></a><span class="st">and I'm a data scientist. Supercalifragilisticexpialidocious.</span></span>
<span id="cb39-4"><a href="#cb39-4"></a><span class="st">"""</span>)</span>
<span id="cb39-5"><a href="#cb39-5"></a></span>
<span id="cb39-6"><a href="#cb39-6"></a><span class="bu">print</span>(<span class="ss">f"Encoded text: </span><span class="sc">{</span><span class="st">', '</span><span class="sc">.</span>join([<span class="bu">str</span>(i) <span class="cf">for</span> i <span class="kw">in</span> encoded])<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb39-7"><a href="#cb39-7"></a>decoded <span class="op">=</span> [pipe.tokenizer.decode(i) <span class="cf">for</span> i <span class="kw">in</span> encoded]</span>
<span id="cb39-8"><a href="#cb39-8"></a><span class="bu">print</span>(<span class="ss">f"Decoded text: </span><span class="sc">{</span><span class="st">', '</span><span class="sc">.</span>join([<span class="bu">str</span>(i) <span class="cf">for</span> i <span class="kw">in</span> decoded])<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Encoded text: 8087, 107, 600, 442, 117, 8303, 26159, 10539, 111, 125, 131, 208, 114, 335, 9732, 107, 2422, 62955, 40972, 4935, 10855, 39053, 7434, 15398, 35898, 107, 1
Decoded text: Hello, ., My, name, is, Greg, Stra, bel, and, I, ', m, a, data, scientist, ., Super, cali, frag, il, istic, exp, ial, ido, cious, ., &lt;/s&gt;</code></pre>
</div>
</div>
<h3 id="summarizing-the-example-article">Summarizing the example article</h3>
<div class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1"></a>sample_text <span class="op">=</span> dataset[<span class="st">"test"</span>][sample_idx][<span class="st">"article"</span>][:<span class="dv">2000</span>]</span>
<span id="cb41-2"><a href="#cb41-2"></a></span>
<span id="cb41-3"><a href="#cb41-3"></a>pipe_out <span class="op">=</span> pipe(sample_text)</span>
<span id="cb41-4"><a href="#cb41-4"></a>summary <span class="op">=</span> pipe_out[<span class="dv">0</span>][<span class="st">"summary_text"</span>].replace(<span class="st">" .&lt;n&gt;"</span>, <span class="st">".</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb41-5"><a href="#cb41-5"></a><span class="bu">print</span>(summary)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The Palestinian Authority officially becomes the 123rd member of the International Criminal Court.
The move gives the court jurisdiction over alleged crimes in Palestinian territories .</code></pre>
</div>
</div>
</section>
<section id="wrap-up" class="slide level2">
<h2>Wrap Up</h2>
<p>This has been a very short introduction to Natural Language Processing, touching on only a small subset of NLP:</p>
<ul>
<li>Common text preprocessing steps</li>
<li>Classical NLP with bag-of-words, n-grams and logistic regression</li>
<li>State-of-the-art NLP with Transformers.</li>
</ul>
<p>Additional resources:</p>
<ul>
<li><a href="https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf">Speech and Language Processing 3rd Edition</a></li>
<li><a href="https://www.amazon.com/Natural-Language-Processing-Transformers-Applications/dp/1098103246/ref=sr_1_1?keywords=natural+language+processing+with+transformers&amp;qid=1658334866&amp;sprefix=natural+language%2Caps%2C113&amp;sr=8-1">Natural Language Processing with Transformers: Building Language Applications with Hugging Face</a></li>
</ul>
<div class="footer footer-default">

</div>
</section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="Intro to NLP_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="Intro to NLP_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="Intro to NLP_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="Intro to NLP_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="Intro to NLP_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="Intro to NLP_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="Intro to NLP_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="Intro to NLP_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="Intro to NLP_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="Intro to NLP_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'smaller': true,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': true,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: false,

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1200,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        // Bounds for smallest/largest possible scale to apply to content
        minScale: 0.5,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script type="application/vnd.jupyter.widget-state+json">
    {"state":{"001465fdf47b4597ad65fcb99db7299b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"23aed4c6e5e74ff08394966f23f09a1c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_85c660d68158497aafba7670bc3d6442","placeholder":"​","style":"IPY_MODEL_001465fdf47b4597ad65fcb99db7299b","value":" 3/3 [00:00&lt;00:00,  3.42it/s]"}},"33a6635f98b3423ab18c0193f5569e1c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_815008ae89544623bc1d1fa912801296","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4f1143b85c9e4aa8b31ff558851a5d91","value":3}},"4f1143b85c9e4aa8b31ff558851a5d91":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5b2021a5f57a412ca3d2a4c79a4f39c8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"815008ae89544623bc1d1fa912801296":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"85c660d68158497aafba7670bc3d6442":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa9e87af13844db88baff020f2a1df16":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b2021a5f57a412ca3d2a4c79a4f39c8","placeholder":"​","style":"IPY_MODEL_cfcaeec476e1477380365c61656850df","value":"100%"}},"ce35a98429f34742aa75db6e30229887":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cfcaeec476e1477380365c61656850df":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f2b3fff2b9f24213b32f395bcce25090":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_aa9e87af13844db88baff020f2a1df16","IPY_MODEL_33a6635f98b3423ab18c0193f5569e1c","IPY_MODEL_23aed4c6e5e74ff08394966f23f09a1c"],"layout":"IPY_MODEL_ce35a98429f34742aa75db6e30229887"}}},"version_major":2,"version_minor":0}
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        target: function(trigger) {
          return trigger.previousElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        setTimeout(function() {
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
      function tippyHover(el, contentFn) {
        const config = {
          allowHTML: true,
          content: contentFn,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'quarto-reveal',
          placement: 'bottom-start'
        };
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          let href = ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          return note.innerHTML;
        });
      }
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const cites = ref.parentNode.getAttribute('data-cites').split(' ');
        tippyHover(ref, function() {
          var popup = window.document.createElement('div');
          cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    });
    </script>
    

</body></html>